{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import helper\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "from IPython.display import display\n",
    "import time\n",
    "import sys\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Load and view Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bromwell high is a cartoon comedy . it ran at the same time as some other programs about school life  such as  teachers  . my   years in the teaching profession lead me to believe that bromwell high  s satire is much closer to reality than is  teachers  . the scramble to survive financially  the insightful students who can see right through their pathetic teachers  pomp  the pettiness of the whole situation  all remind me of the schools i knew and their students . when i saw the episode in which a student repeatedly tried to burn down the school  i immediately recalled . . . . . . . . . at . . . . . . . . . . high . a classic line inspector i  m here to sack one of your teachers . student welcome to bromwell high . i expect that many adults of my age think that bromwell high is far fetched . what a pity that it isn  t    \n",
      "\n",
      "positive\n"
     ]
    }
   ],
   "source": [
    "reviews, labels = helper.load_data()\n",
    "print(reviews[0],'\\n')\n",
    "print(labels[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# understand dataset\n",
    "- in this part i will try to find what is the reason for a review to be positive and what is the words that appear in positive or negative review\n",
    "- we want to know which words appear in negative and positive\n",
    "- positive words and negative_words are counter objects will have count of each word exist either in negative reviews or positive reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no of positive reviews=12500\n",
      "no of negative reviews=12500\n"
     ]
    }
   ],
   "source": [
    "no_positive_rev = labels.count('positive')\n",
    "no_negative_rev = len(labels) - no_positive_rev\n",
    "print(f\"no of positive reviews={no_positive_rev}\")\n",
    "print(f\"no of negative reviews={no_negative_rev}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['positive', 'negative', 'positive', 'negative', 'positive', 'negative', 'positive', 'negative', 'positive', 'negative', 'positive', 'negative', 'positive', 'negative', 'positive', 'negative', 'positive', 'negative', 'positive', 'negative', 'positive', 'negative', 'positive', 'negative', 'positive']\n"
     ]
    }
   ],
   "source": [
    "print(labels[:25])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "\n",
    "- **data is evenly distributed data even indices for positive and  odd for negative**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "positive_words_cnt = Counter()\n",
    "negative_words_cnt = Counter()\n",
    "all_words_cnt = Counter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "for i in range(len(reviews)):\n",
    "    words = reviews[i].split(\" \")\n",
    "    for word in words:\n",
    "        if labels[i] == 'positive':\n",
    "            positive_words_cnt[word] += 1\n",
    "        else:\n",
    "            negative_words_cnt[word] += 1\n",
    "        all_words_cnt[word] += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## showing the most common words appear in positive and also in negative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "[('', 550468),\n ('the', 173324),\n ('.', 159654),\n ('and', 89722),\n ('a', 83688),\n ('of', 76855),\n ('to', 66746),\n ('is', 57245),\n ('in', 50215),\n ('br', 49235),\n ('it', 48025),\n ('i', 40743),\n ('that', 35630),\n ('this', 35080),\n ('s', 33815),\n ('as', 26308),\n ('with', 23247),\n ('for', 22416),\n ('was', 21917),\n ('film', 20937),\n ('but', 20822),\n ('movie', 19074),\n ('his', 17227),\n ('on', 17008),\n ('you', 16681),\n ('he', 16282),\n ('are', 14807),\n ('not', 14272),\n ('t', 13720),\n ('one', 13655),\n ('have', 12587),\n ('be', 12416),\n ('by', 11997),\n ('all', 11942),\n ('who', 11464),\n ('an', 11294),\n ('at', 11234),\n ('from', 10767),\n ('her', 10474),\n ('they', 9895),\n ('has', 9186),\n ('so', 9154),\n ('like', 9038),\n ('about', 8313),\n ('very', 8305),\n ('out', 8134),\n ('there', 8057),\n ('she', 7779),\n ('what', 7737),\n ('or', 7732),\n ('good', 7720),\n ('more', 7521),\n ('when', 7456),\n ('some', 7441),\n ('if', 7285),\n ('just', 7152),\n ('can', 7001),\n ('story', 6780),\n ('time', 6515),\n ('my', 6488),\n ('great', 6419),\n ('well', 6405),\n ('up', 6321),\n ('which', 6267),\n ('their', 6107),\n ('see', 6026),\n ('also', 5550),\n ('we', 5531),\n ('really', 5476),\n ('would', 5400),\n ('will', 5218),\n ('me', 5167),\n ('had', 5148),\n ('only', 5137),\n ('him', 5018),\n ('even', 4964),\n ('most', 4864),\n ('other', 4858),\n ('were', 4782),\n ('first', 4755),\n ('than', 4736),\n ('much', 4685),\n ('its', 4622),\n ('no', 4574),\n ('into', 4544),\n ('people', 4479),\n ('best', 4319),\n ('love', 4301),\n ('get', 4272),\n ('how', 4213),\n ('life', 4199),\n ('been', 4189),\n ('because', 4079),\n ('way', 4036),\n ('do', 3941),\n ('made', 3823),\n ('films', 3813),\n ('them', 3805),\n ('after', 3800),\n ('many', 3766)]"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "[('', 561462),\n ('.', 167538),\n ('the', 163389),\n ('a', 79321),\n ('and', 74385),\n ('of', 69009),\n ('to', 68974),\n ('br', 52637),\n ('is', 50083),\n ('it', 48327),\n ('i', 46880),\n ('in', 43753),\n ('this', 40920),\n ('that', 37615),\n ('s', 31546),\n ('was', 26291),\n ('movie', 24965),\n ('for', 21927),\n ('but', 21781),\n ('with', 20878),\n ('as', 20625),\n ('t', 20361),\n ('film', 19218),\n ('you', 17549),\n ('on', 17192),\n ('not', 16354),\n ('have', 15144),\n ('are', 14623),\n ('be', 14541),\n ('he', 13856),\n ('one', 13134),\n ('they', 13011),\n ('at', 12279),\n ('his', 12147),\n ('all', 12036),\n ('so', 11463),\n ('like', 11238),\n ('there', 10775),\n ('just', 10619),\n ('by', 10549),\n ('or', 10272),\n ('an', 10266),\n ('who', 9969),\n ('from', 9731),\n ('if', 9518),\n ('about', 9061),\n ('out', 8979),\n ('what', 8422),\n ('some', 8306),\n ('no', 8143),\n ('her', 7947),\n ('even', 7687),\n ('can', 7653),\n ('has', 7604),\n ('good', 7423),\n ('bad', 7401),\n ('would', 7036),\n ('up', 6970),\n ('only', 6781),\n ('more', 6730),\n ('when', 6726),\n ('she', 6444),\n ('really', 6262),\n ('time', 6209),\n ('had', 6142),\n ('my', 6015),\n ('were', 6001),\n ('which', 5780),\n ('very', 5764),\n ('me', 5606),\n ('see', 5452),\n ('don', 5336),\n ('we', 5328),\n ('their', 5278),\n ('do', 5236),\n ('story', 5208),\n ('than', 5183),\n ('been', 5100),\n ('much', 5078),\n ('get', 5037),\n ('because', 4966),\n ('people', 4806),\n ('then', 4761),\n ('make', 4722),\n ('how', 4688),\n ('could', 4686),\n ('any', 4658),\n ('into', 4567),\n ('made', 4541),\n ('first', 4306),\n ('other', 4305),\n ('well', 4254),\n ('too', 4174),\n ('them', 4165),\n ('plot', 4154),\n ('movies', 4080),\n ('acting', 4056),\n ('will', 3993),\n ('way', 3989),\n ('most', 3919)]"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(positive_words_cnt.most_common(100))\n",
    "display(negative_words_cnt.most_common(100))"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    " <font size=\"5\" style=\"font-weight: bold;\">we have now the count of each word in both positive and negative, but we want the words that appear in positive only and doesn't appear in negative and vice versa </font>"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ![word_importance.png](notebook-images/word_importance.png)\n",
    "away for this to calculate the ration of the word log(positive_cnt/negative_cnt)\n",
    "- if count of the word in positive reviews nearly equal in negative reviews\n",
    "    - the result will be near to zero\n",
    "- if it appears in positive\n",
    "    - the result will be large number greater than 0  +ve\n",
    "- if it appears more in negative\n",
    "    - the result will be large negative number -ve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "words_pos_neg_ratio = Counter()\n",
    "for word, cnt in all_words_cnt.items():\n",
    "    if cnt < 100:  #doesn't appear much\n",
    "        continue\n",
    "    words_pos_neg_ratio[word] = np.log((positive_words_cnt[word] + 1) / (negative_words_cnt[word] + 1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "common words\n",
      "pos and negative ratio for 'the' 0.05902846378582202\n",
      "pos and negative ratio for 'is' 0.13365617519106257\n",
      "\n",
      "Postive words\n",
      "pos and negative ratio for 'wonderful' 1.56527119000835\n",
      "pos and negative ratio for 'amazing' 1.3929263134236418\n",
      "\n",
      "negative words\n",
      "pos and negative ratio for 'bad' -1.3556946609378695\n",
      "pos and negative ratio for 'worse' -1.7327361287785938\n"
     ]
    }
   ],
   "source": [
    "print(\"common words\")\n",
    "print(f\"pos and negative ratio for 'the' {words_pos_neg_ratio['the']}\")\n",
    "print(f\"pos and negative ratio for 'is' {words_pos_neg_ratio['is']}\")\n",
    "print(\"\\nPostive words\")\n",
    "\n",
    "print(f\"pos and negative ratio for 'wonderful' {words_pos_neg_ratio['wonderful']}\")\n",
    "print(f\"pos and negative ratio for 'amazing' {words_pos_neg_ratio['amazing']}\")\n",
    "\n",
    "print(\"\\nnegative words\")\n",
    "print(f\"pos and negative ratio for 'bad' {words_pos_neg_ratio['bad']}\")\n",
    "print(f\"pos and negative ratio for 'worse' {words_pos_neg_ratio['worse']}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "common words ~ 0\n",
    "positive words >1\n",
    "negative words <-1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positive most common\n"
     ]
    },
    {
     "data": {
      "text/plain": "[('edie', 4.700480365792417),\n ('paulie', 4.085976312551584),\n ('felix', 3.1612467120315646),\n ('polanski', 2.833213344056216),\n ('matthau', 2.8134107167600364),\n ('victoria', 2.6855773452501515),\n ('mildred', 2.6119063405493077),\n ('gandhi', 2.5477075510270306),\n ('flawless', 2.4595888418037104),\n ('superbly', 2.268683541318364),\n ('perfection', 2.1671471220989416),\n ('astaire', 2.1484344131667874),\n ('captures', 2.0438143640366846),\n ('voight', 2.0402208285265546),\n ('wonderfully', 2.02537432040956),\n ('powell', 1.9836504770381602),\n ('brosnan', 1.9636097261547143),\n ('lily', 1.9289605907415401),\n ('bakshi', 1.911718784307034),\n ('lincoln', 1.9079309009900969),\n ('refreshing', 1.8607523407150064),\n ('lemmon', 1.8562979903656263),\n ('breathtaking', 1.8549383708495866),\n ('bourne', 1.8538912503350613),\n ('flynn', 1.807507826196194),\n ('delightful', 1.8044984950054848),\n ('andrews', 1.7841548698428356),\n ('homer', 1.7805861686299298),\n ('soccer', 1.7692866133759964),\n ('beautifully', 1.76537271405486),\n ('lumet', 1.7578579175523736),\n ('elvira', 1.7473077066572211),\n ('underrated', 1.7247487589450947),\n ('gripping', 1.7243181884325225),\n ('superb', 1.71090737259896),\n ('delight', 1.6789639750827108),\n ('sadness', 1.6739764335716716),\n ('welles', 1.6724127115954888),\n ('sinatra', 1.6438393391514328),\n ('touching', 1.6399534563600509),\n ('timeless', 1.6389967146756448),\n ('macy', 1.6326947745983675),\n ('unforgettable', 1.6259672143853108),\n ('favorites', 1.6222586008631619),\n ('hartley', 1.6211339521972916),\n ('extraordinary', 1.6163107917218624),\n ('sullivan', 1.6156684621847364),\n ('stewart', 1.6145530131008707),\n ('brilliantly', 1.5998684614179497),\n ('friendship', 1.5720115069149834),\n ('palma', 1.5664205273504097),\n ('wonderful', 1.56527119000835),\n ('magnificent', 1.5512559570513647),\n ('finest', 1.550597412411167),\n ('ritter', 1.5493339883643948),\n ('jackie', 1.5491181222005777),\n ('tremendous', 1.5279448781829175),\n ('freedom', 1.5141277326297755),\n ('fantastic', 1.5063736090366242),\n ('terrific', 1.505482878385009),\n ('sidney', 1.5007047122976347),\n ('pleasantly', 1.4992347723004862),\n ('mann', 1.4992347723004862),\n ('noir', 1.4968362355197145),\n ('outstanding', 1.493925025312256),\n ('nancy', 1.4934389985712184),\n ('marie', 1.4873904779912595),\n ('marvelous', 1.4816045409242156),\n ('ruth', 1.4696759700589417),\n ('excellent', 1.4653478511838256),\n ('stanwyck', 1.4488147181012245),\n ('widmark', 1.442989704796436),\n ('splendid', 1.4370666864933137),\n ('chan', 1.4291143583028187),\n ('exceptional', 1.4284947156102672),\n ('tender', 1.423108334242607),\n ('gentle', 1.418382675671391),\n ('poignant', 1.410199881973445),\n ('gem', 1.3966571481554373),\n ('captivating', 1.3966571481554373),\n ('fisher', 1.3958638121360414),\n ('davies', 1.3958638121360414),\n ('chilling', 1.3936204012119635),\n ('amazing', 1.3929263134236418),\n ('darker', 1.3758230612525952),\n ('april', 1.3621968095408301),\n ('blake', 1.3531421538029902),\n ('kelly', 1.3490940774338),\n ('overlooked', 1.3388921222253067),\n ('ralph', 1.3366974199805186),\n ('bette', 1.3237740041385566),\n ('hoffman', 1.3217558399823195),\n ('cole', 1.3217558399823195),\n ('shines', 1.3133875903118029),\n ('powerful', 1.3020133612531182),\n ('notch', 1.3013949173334043),\n ('winters', 1.295322582914164),\n ('pitt', 1.292768303109067),\n ('remarkable', 1.2924756059022358),\n ('vivid', 1.2878542883066382)]"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"Positive most common\")\n",
    "display(words_pos_neg_ratio.most_common(100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "words like magnificent and amazing, wonderful, appear from top 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Negative most common\n"
     ]
    },
    {
     "data": {
      "text/plain": "[('boll', -4.276666119016055),\n ('uwe', -3.9318256327243257),\n ('seagal', -3.4210000089583352),\n ('unwatchable', -3.0349529867072724),\n ('stinker', -2.9856819377004897),\n ('mst', -2.8449093838194073),\n ('incoherent', -2.803360380906535),\n ('unfunny', -2.635081181235619),\n ('waste', -2.6093342281630525),\n ('blah', -2.501435951739211),\n ('pointless', -2.430613567421338),\n ('horrid', -2.379546134130174),\n ('atrocious', -2.36528368720961),\n ('redeeming', -2.3331477434042123),\n ('worst', -2.283027494964281),\n ('prom', -2.2655438213136967),\n ('drivel', -2.26002547857525),\n ('lousy', -2.2587824703356527),\n ('laughable', -2.2396712675834767),\n ('awful', -2.22125951150762),\n ('poorly', -2.206570439754457),\n ('remotely', -2.145931282948669),\n ('wasting', -2.1400661634962708),\n ('existent', -2.02537432040956),\n ('lame', -1.970717622759581),\n ('sucks', -1.9580806846755685),\n ('insult', -1.94060509682562),\n ('boredom', -1.9379419794061366),\n ('miserably', -1.927891643552635),\n ('uninspired', -1.9187591599893623),\n ('uninteresting', -1.9066894359020319),\n ('horrible', -1.9038282036209997),\n ('pathetic', -1.8841327893590702),\n ('godzilla', -1.8827312474337816),\n ('unconvincing', -1.8748743759385615),\n ('amateurish', -1.8744511850731684),\n ('appalling', -1.8672670217362002),\n ('gadget', -1.8666607774011728),\n ('idiotic', -1.8484548129046003),\n ('unintentional', -1.845826690498331),\n ('stupidity', -1.836211231798889),\n ('wasted', -1.82537608002704),\n ('crap', -1.821206670554356),\n ('cardboard', -1.791759469228055),\n ('tedious', -1.7707060600302227),\n ('insulting', -1.7650912221458936),\n ('dreadful', -1.7443572303334711),\n ('badly', -1.743474228068593),\n ('worse', -1.7327361287785938),\n ('terrible', -1.7250680947293828),\n ('suck', -1.7147984280919266),\n ('dire', -1.709521370991083),\n ('mess', -1.6801454845983868),\n ('embarrassing', -1.6739764335716716),\n ('garbage', -1.6731199024700552),\n ('stupid', -1.6516020715417972),\n ('pile', -1.6384254493073527),\n ('vampires', -1.5950491749820006),\n ('ashamed', -1.5869650565820417),\n ('dull', -1.5775048653310912),\n ('worthless', -1.5723966407537513),\n ('avoid', -1.568304924286708),\n ('wooden', -1.552685095841651),\n ('inept', -1.5496194172231903),\n ('forgettable', -1.5248805244060373),\n ('crappy', -1.5141277326297755),\n ('ridiculous', -1.502813972729502),\n ('bat', -1.5004867286455454),\n ('fulci', -1.498772344546581),\n ('excuse', -1.4968362355197145),\n ('whatsoever', -1.494616588272045),\n ('rubbish', -1.48870936654796),\n ('boring', -1.4879804697340995),\n ('unbelievably', -1.4733057381095205),\n ('junk', -1.466337068793427),\n ('turkey', -1.4593194961347804),\n ('shark', -1.449095262358921),\n ('flop', -1.4441139320087168),\n ('topless', -1.4350845252893227),\n ('useless', -1.4350845252893227),\n ('ripped', -1.4315509527080115),\n ('ridiculously', -1.4307461236907244),\n ('embarrassed', -1.4246132254220272),\n ('seed', -1.423108334242607),\n ('costs', -1.418382675671391),\n ('dumb', -1.415281897993143),\n ('bother', -1.4116121691041805),\n ('rambo', -1.3971052772241062),\n ('horrendous', -1.3955110162248145),\n ('horribly', -1.3920914788042167),\n ('plastic', -1.3862943611198906),\n ('hideous', -1.3862943611198906),\n ('fest', -1.3773256911371303),\n ('disjointed', -1.374318170073175),\n ('ludicrous', -1.3723081191451507),\n ('bland', -1.3682758556172123),\n ('annoying', -1.3614791920001665),\n ('unintentionally', -1.3591433720539396),\n ('mildly', -1.3581234841531944),\n ('obnoxious', -1.3564413979702095)]"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"Negative most common\")\n",
    "display(list(reversed(words_pos_neg_ratio.most_common()))[0:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max value:4.700480365792417\n",
      "min value:-4.276666119016055\n",
      "Words count:74074\n"
     ]
    }
   ],
   "source": [
    "print(f\"max value:{helper.max_counter_value(words_pos_neg_ratio)}\")\n",
    "print(f\"min value:{helper.min_counter_value(words_pos_neg_ratio)}\")\n",
    "print(f\"Words count:{len(all_words_cnt)}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Creating the Model\n",
    "- ***Network structure***\n",
    "![network](notebook-images/network.png)\n",
    "- ---\n",
    "- after viewing the data existence of some words in a review could classify it to positive or negative so our network will be as follows\n",
    "- we need to represent the data input as vector of zeros and ones when the word exists in a review it will have value one in the index of the word in the vector\n",
    "- we need to ignore irrelevant words that appear in both positive and negative via **threshold** in pos_negative_ratio"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## feedforward equations\n",
    "\n",
    "![feedforward](notebook-images/feedforward.PNG)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## backpropagation equations\n",
    "![equations-png](notebook-images/backpropagation-equations.PNG)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class Model:\n",
    "    def __init__(self, reviews, labels, hidden_nodes=128, threshold=0.2,load_weights=False):\n",
    "        \"\"\"\n",
    "\n",
    "        :param reviews: reviews list to make a  vocab list to be the input layer of the nn\n",
    "        :param labels: label for each review\n",
    "        :param hidden_nodes: no of hidden nodes in the hidden layer\n",
    "        :param threshold: threshold of pos_negative_ratio for a word to take it of not\n",
    "        :param load_weights: flag to load a previous weights from model_weights dir if it exists\n",
    "        \"\"\"\n",
    "        np.random.seed(69)\n",
    "        self.hidden_nodes = hidden_nodes\n",
    "        self.threshold = threshold\n",
    "\n",
    "        filtered_vocab = self.data_preprocessing(reviews, labels)\n",
    "        self.word2idx = {}\n",
    "        for idx, word in enumerate(filtered_vocab):\n",
    "            self.word2idx[word] = idx\n",
    "        self.input_size = len(filtered_vocab)\n",
    "        self.fc1 = helper.NNLiner(input_size=self.input_size, output_size=self.hidden_nodes)\n",
    "        self.fc2 = helper.NNLiner(input_size=self.hidden_nodes, output_size=1)\n",
    "\n",
    "        self.sigmoid = lambda x: 1.0 / (1.0 + (np.exp(-x)))\n",
    "\n",
    "        if load_weights:\n",
    "            try:\n",
    "                self.load_weights()\n",
    "                print(\"model loaded with saved weights\")\n",
    "            except:\n",
    "                print(\"model loaded with random weights\")\n",
    "    def data_preprocessing(self, reviews, labels):\n",
    "        # calculate the importance of each word using pos\n",
    "        filtered_reviews_vocab = set()\n",
    "        positive_words_cnt = Counter()\n",
    "        negative_words_cnt = Counter()\n",
    "        reviews_vocab = set()\n",
    "\n",
    "        for i in range(len(reviews)):\n",
    "            words = reviews[i].split(' ')\n",
    "            for word in words:\n",
    "                if labels[i] == 'positive':\n",
    "                    positive_words_cnt[word] += 1\n",
    "                else:\n",
    "                    negative_words_cnt[word] += 1\n",
    "                reviews_vocab.add(word)\n",
    "\n",
    "        for word in reviews_vocab:\n",
    "            word_ratio = np.log((positive_words_cnt[word] + 1) / (negative_words_cnt[word] + 1))\n",
    "            if abs(word_ratio) >= self.threshold:\n",
    "                filtered_reviews_vocab.add(word)\n",
    "        return filtered_reviews_vocab\n",
    "\n",
    "    def __call__(self, review):\n",
    "        a0 = self.review_encode(review)\n",
    "\n",
    "        z1 = self.fc1(a0)\n",
    "        # a1 = self.sigmoid(z1)\n",
    "        a1=z1\n",
    "        z2 = self.fc2(a1)\n",
    "        a2 = self.sigmoid(z2)\n",
    "        return a2\n",
    "\n",
    "    def review_encode(self, review):\n",
    "        \"\"\"\n",
    "        this method takes review and convert it to vocab vector of zeros and ones according to our saved vocab\n",
    "        each word has index in the vector saved in word2index vector if the word in review exist in word2index we set it to one in the input vector\n",
    "\n",
    "        :param review: string review\n",
    "        :return: input vector of zeros and ones to feed it to fc1\n",
    "        \"\"\"\n",
    "        a0 = np.zeros(self.input_size, float)\n",
    "        for word in review.split(' '):\n",
    "            if word in self.word2idx:\n",
    "                a0[self.word2idx[word]] = 1.0\n",
    "        return np.expand_dims(a0, axis=0) # expand dim to make row vector --> [[x1,x2,x3....]]\n",
    "\n",
    "    def train(self,epochs,learning_rate, train_reviews, train_labels,test_reviews,test_labels):\n",
    "\n",
    "        assert (len(train_reviews) == len(train_labels))\n",
    "        for e in range(epochs):\n",
    "            data_size=len(train_labels)\n",
    "            correct_cnt = 0\n",
    "            forward_cnt = 0\n",
    "            error_sum=0\n",
    "            mean_error=0\n",
    "            print_every=int(len(train_labels)*0.1) #print log every 10 percent\n",
    "            start_time = time.time()\n",
    "            train_accuracy=0\n",
    "            print(f\"\\nepoch {e+1}\\n\")\n",
    "            for review, label in zip(train_reviews, train_labels):\n",
    "                forward_cnt += 1\n",
    "                if label == 'positive':\n",
    "                    actual_output = 1\n",
    "                else:\n",
    "                    actual_output = 0\n",
    "\n",
    "                #------------------------------Feed-Forward----------------------------------------#\n",
    "                a0 = self.review_encode(review)\n",
    "                z1 = self.fc1(a0)\n",
    "                a1 = z1 #linear , no activation function\n",
    "\n",
    "                z2 = self.fc2(a1)\n",
    "                a2 = self.sigmoid(z2)\n",
    "\n",
    "                predicted = 1\n",
    "                if a2 < 0.5:\n",
    "                    predicted = 0\n",
    "                if predicted == actual_output:\n",
    "                    correct_cnt += 1\n",
    "                #d --> derivative\n",
    "\n",
    "                error =-(actual_output*np.log(a2) + (1-actual_output)*np.log(1-a2))  # 2d matrix [[error]]\n",
    "                error=error[0][0]\n",
    "                error_sum+=error\n",
    "                mean_error=(error_sum/forward_cnt)\n",
    "                #______________________________Backpropagation_______________________________________#\n",
    "                #Ti --> error term of layer i\n",
    "\n",
    "                # _______________delta_w2____________________\n",
    "                #         |  error term 2  |\n",
    "                # dE/dW2 = dE/a2 * da2/dz2 * dz2/dw2 = T2 * da2/dz2\n",
    "                de_da2 = -(actual_output/a2) + (1-actual_output)/(1-a2)\n",
    "                da2_dz2 = a2 * (1 - a2)  # derivative of sigmoid is sigmoid*(1-sigmoid)\n",
    "\n",
    "                t2 = de_da2 * da2_dz2  # --> dimensions (1x2) = dimensions of a2\n",
    "\n",
    "                de_dw2 = (a1.transpose()) @ t2  #--> (3*1)@ (1*2) = (3x2) same like w2 size\n",
    "                #-----------------------------------------------\n",
    "\n",
    "                # _______________delta_w2____________________\n",
    "                #dE/dW1=dE/a2 * da2/dz2 * dz2/da1 * da1/dz1 * dz1/dw1 = t2 * dz2/da1 * da1/dz1 * dz1/dw1\n",
    "\n",
    "                #dz2/da1= d/da1 (a1@W2 + b2) = W2\n",
    "                # da1_z1=a1*(1-a1)\n",
    "                da1_z1 = 1\n",
    "                #dz1/dw1=a0\n",
    "\n",
    "                t1 = t2 @ self.fc2.weights.T * da1_z1\n",
    "                de_dw1 = a0.transpose() @ t1\n",
    "\n",
    "                #------------ update weights ------------------#\n",
    "                self.fc1.weights -= (de_dw1 * learning_rate)\n",
    "                self.fc2.weights -= (de_dw2 * learning_rate)\n",
    "                #-----------------------------------------------\n",
    "\n",
    "                #------------ update bias ------------------#\n",
    "                self.fc1.bias-=(t1*learning_rate)\n",
    "                self.fc2.bias-=(t2*learning_rate)\n",
    "                 #-----------------------------------------------\n",
    "\n",
    "                elapsed_time = float(time.time() - start_time)\n",
    "                reviews_per_second = forward_cnt / elapsed_time if elapsed_time > 0 else 0\n",
    "\n",
    "                train_accuracy=(correct_cnt * 100) / float(forward_cnt + 1)\n",
    "\n",
    "                sys.stdout.write(\"\\rProgress:\" + str(100 * forward_cnt / float(data_size))[:4]\n",
    "                                 + \"% Speed(reviews/sec):\" + str(reviews_per_second)[0:5]\n",
    "                                 + \" #Correct:\" + str(correct_cnt) + \" #Trained:\" + str(forward_cnt + 1)\n",
    "                                 + \" Training Accuracy:\" + str(train_accuracy)[:4] + \"%\"+\n",
    "                                 \" Train loss: \"+str(mean_error)[:8])\n",
    "                if forward_cnt % print_every == 0:\n",
    "                    print(\"\")\n",
    "            print(\"\\nTesting\\n\")\n",
    "            #------------------------------------------------------Testing----------------------------------------------------------#\n",
    "            correct_cnt = 0\n",
    "            forward_cnt = 0\n",
    "            start_time = time.time()\n",
    "            data_size=len(test_labels)\n",
    "            print_every=int(len(test_labels)*0.1) #print log every 10 percent\n",
    "            test_accuracy=0\n",
    "            error_sum=0\n",
    "            mean_error=0\n",
    "\n",
    "            for review, label in zip(test_reviews, test_labels):\n",
    "                forward_cnt += 1\n",
    "                if label == 'positive':\n",
    "                    actual_output = 1\n",
    "                else:\n",
    "                    actual_output = 0\n",
    "                #------------------------------Feed-Forward----------------------------------------#\n",
    "                a2=self(review)\n",
    "                error =-(actual_output*np.log(a2) + (1-actual_output)*np.log(1-a2))  # 2d matrix [[error]]\n",
    "                error=error[0][0]\n",
    "                error_sum+=error\n",
    "                mean_error=(error_sum/forward_cnt)\n",
    "\n",
    "                predicted = 1\n",
    "                if a2 < 0.5:\n",
    "                    predicted = 0\n",
    "                if predicted == actual_output:\n",
    "                    correct_cnt += 1\n",
    "                elapsed_time = float(time.time() - start_time)\n",
    "                reviews_per_second = forward_cnt / elapsed_time if elapsed_time > 0 else 0\n",
    "                test_accuracy=(correct_cnt * 100) / float(forward_cnt + 1)\n",
    "\n",
    "\n",
    "                sys.stdout.write(\"\\rProgress:\" + str(100 * forward_cnt / float(data_size))[:4]\n",
    "                                 + \"% Speed(reviews/sec):\" + str(reviews_per_second)[0:5]\n",
    "                                 + \" #Correct:\" + str(correct_cnt) + \" #Tested:\" + str(forward_cnt + 1)\n",
    "                                 + \" Testing Accuracy:\" + str(test_accuracy)[:4] + \"%\"+\n",
    "                                 \" Test loss: \"+str(mean_error)[:10])\n",
    "                if forward_cnt % print_every == 0:\n",
    "                    print(\"\")\n",
    "            if train_accuracy>test_accuracy:\n",
    "                print(\"\\nWarning overfitting training loop will break\")\n",
    "                break\n",
    "            else :\n",
    "                self.save_weights()\n",
    "    def save_weights(self):\n",
    "        np.save('model_weights/fc1.npy',self.fc1.weights)\n",
    "        np.save('model_weights/fc2.npy',self.fc2.weights)\n",
    "    def load_weights(self):\n",
    "        try:\n",
    "            self.fc1.weights=np.load('model_weights/fc1.npy')\n",
    "        except FileNotFoundError:\n",
    "            raise Exception(f\"can't load fc1 weights please check that fc1 weights exists in 'model_weights/fc1.npy' \")\n",
    "        try:\n",
    "            self.fc2.weights=np.load('model_weights/fc2.npy')\n",
    "        except FileNotFoundError:\n",
    "            raise Exception(f\"can't load fc2 weights please check that fc2 weights exists in 'model_weights/fc2.npy' \")\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "model = Model(reviews, labels, hidden_nodes=25, threshold=0.15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.49793558]]\n",
      "positive\n"
     ]
    }
   ],
   "source": [
    "\n",
    "x = 0\n",
    "print(model.__call__(reviews[x]))\n",
    "print(labels[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total records=25000\n",
      "train_size=18750\n",
      "test_size=6250\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "test_percentage = 0.25\n",
    "\n",
    "train_reviews,test_reviews,train_labels,test_labels=train_test_split(reviews,labels,test_size=test_percentage,random_state=69)\n",
    "data_size = int(len(reviews))\n",
    "\n",
    "print(f\"total records={data_size}\\ntrain_size={len(train_reviews)}\\ntest_size={len(test_reviews)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "epoch 1\n",
      "\n",
      "Progress:10.0% Speed(reviews/sec):24.09 #Correct:1428 #Trained:1876 Training Accuracy:76.1% Train loss: 0.503614\n",
      "Progress:20.0% Speed(reviews/sec):21.73 #Correct:3002 #Trained:3751 Training Accuracy:80.0% Train loss: 0.440667\n",
      "Progress:30.0% Speed(reviews/sec):18.90 #Correct:4591 #Trained:5626 Training Accuracy:81.6% Train loss: 0.410073\n",
      "Progress:40.0% Speed(reviews/sec):18.92 #Correct:6180 #Trained:7501 Training Accuracy:82.3% Train loss: 0.393220\n",
      "Progress:50.0% Speed(reviews/sec):19.76 #Correct:7776 #Trained:9376 Training Accuracy:82.9% Train loss: 0.382807\n",
      "Progress:60.0% Speed(reviews/sec):20.40 #Correct:9413 #Trained:11251 Training Accuracy:83.6% Train loss: 0.369944\n",
      "Progress:70.0% Speed(reviews/sec):20.94 #Correct:11039 #Trained:13126 Training Accuracy:84.1% Train loss: 0.362935\n",
      "Progress:80.0% Speed(reviews/sec):20.68 #Correct:12654 #Trained:15001 Training Accuracy:84.3% Train loss: 0.359024\n",
      "Progress:90.0% Speed(reviews/sec):20.70 #Correct:14292 #Trained:16876 Training Accuracy:84.6% Train loss: 0.353400\n",
      "Progress:100.% Speed(reviews/sec):20.92 #Correct:15918 #Trained:18751 Training Accuracy:84.8% Train loss: 0.349846\n",
      "\n",
      "Testing\n",
      "\n",
      "Progress:10.0% Speed(reviews/sec):121.8 #Correct:554 #Tested:626 Testing Accuracy:88.4% Test loss: 0.28979337\n",
      "Progress:20.0% Speed(reviews/sec):119.8 #Correct:1106 #Tested:1251 Testing Accuracy:88.4% Test loss: 0.28917560\n",
      "Progress:30.0% Speed(reviews/sec):115.6 #Correct:1647 #Tested:1876 Testing Accuracy:87.7% Test loss: 0.29731937\n",
      "Progress:40.0% Speed(reviews/sec):120.1 #Correct:2188 #Tested:2501 Testing Accuracy:87.4% Test loss: 0.30569180\n",
      "Progress:50.0% Speed(reviews/sec):121.8 #Correct:2736 #Tested:3126 Testing Accuracy:87.5% Test loss: 0.30562259\n",
      "Progress:60.0% Speed(reviews/sec):122.2 #Correct:3296 #Tested:3751 Testing Accuracy:87.8% Test loss: 0.29518946\n",
      "Progress:70.0% Speed(reviews/sec):121.7 #Correct:3847 #Tested:4376 Testing Accuracy:87.9% Test loss: 0.29386974\n",
      "Progress:80.0% Speed(reviews/sec):122.9 #Correct:4398 #Tested:5001 Testing Accuracy:87.9% Test loss: 0.29272832\n",
      "Progress:90.0% Speed(reviews/sec):123.8 #Correct:4957 #Tested:5626 Testing Accuracy:88.1% Test loss: 0.29162389\n",
      "Progress:100.% Speed(reviews/sec):124.4 #Correct:5523 #Tested:6251 Testing Accuracy:88.3% Test loss: 0.29090198\n",
      "\n",
      "epoch 2\n",
      "\n",
      "Progress:10.0% Speed(reviews/sec):21.30 #Correct:1687 #Trained:1876 Training Accuracy:89.9% Train loss: 0.250513\n",
      "Progress:20.0% Speed(reviews/sec):22.56 #Correct:3386 #Trained:3751 Training Accuracy:90.2% Train loss: 0.239707\n",
      "Progress:30.0% Speed(reviews/sec):22.34 #Correct:5118 #Trained:5626 Training Accuracy:90.9% Train loss: 0.225017\n",
      "Progress:40.0% Speed(reviews/sec):22.64 #Correct:6838 #Trained:7501 Training Accuracy:91.1% Train loss: 0.221795\n",
      "Progress:50.0% Speed(reviews/sec):23.27 #Correct:8549 #Trained:9376 Training Accuracy:91.1% Train loss: 0.224537\n",
      "Progress:60.0% Speed(reviews/sec):22.85 #Correct:10271 #Trained:11251 Training Accuracy:91.2% Train loss: 0.223360\n",
      "Progress:70.0% Speed(reviews/sec):23.03 #Correct:11989 #Trained:13126 Training Accuracy:91.3% Train loss: 0.224194\n",
      "Progress:80.0% Speed(reviews/sec):23.45 #Correct:13693 #Trained:15001 Training Accuracy:91.2% Train loss: 0.226895\n",
      "Progress:90.0% Speed(reviews/sec):23.38 #Correct:15391 #Trained:16876 Training Accuracy:91.2% Train loss: 0.227185\n",
      "Progress:100.% Speed(reviews/sec):23.70 #Correct:17084 #Trained:18751 Training Accuracy:91.1% Train loss: 0.228899\n",
      "\n",
      "Testing\n",
      "\n",
      "Progress:10.0% Speed(reviews/sec):145.8 #Correct:556 #Tested:626 Testing Accuracy:88.8% Test loss: 0.30560046\n",
      "Progress:20.0% Speed(reviews/sec):107.8 #Correct:1104 #Tested:1251 Testing Accuracy:88.2% Test loss: 0.30493471\n",
      "Progress:30.0% Speed(reviews/sec):119.2 #Correct:1646 #Tested:1876 Testing Accuracy:87.7% Test loss: 0.31030064\n",
      "Progress:40.0% Speed(reviews/sec):127.3 #Correct:2185 #Tested:2501 Testing Accuracy:87.3% Test loss: 0.32106437\n",
      "Progress:50.0% Speed(reviews/sec):132.8 #Correct:2734 #Tested:3126 Testing Accuracy:87.4% Test loss: 0.32198256\n",
      "Progress:60.0% Speed(reviews/sec):136.2 #Correct:3298 #Tested:3751 Testing Accuracy:87.9% Test loss: 0.30830502\n",
      "Progress:70.0% Speed(reviews/sec):136.9 #Correct:3854 #Tested:4376 Testing Accuracy:88.0% Test loss: 0.30450755\n",
      "Progress:80.0% Speed(reviews/sec):138.7 #Correct:4406 #Tested:5001 Testing Accuracy:88.1% Test loss: 0.30335881\n",
      "Progress:90.0% Speed(reviews/sec):141.1 #Correct:4963 #Tested:5626 Testing Accuracy:88.2% Test loss: 0.30069573\n",
      "Progress:100.% Speed(reviews/sec):142.1 #Correct:5527 #Tested:6251 Testing Accuracy:88.4% Test loss: 0.29950025\n",
      "\n",
      "Warning overfitting training loop will break\n"
     ]
    }
   ],
   "source": [
    "model.train(epochs=5,learning_rate=0.01, train_reviews=train_reviews, train_labels=train_labels,test_reviews=test_reviews,test_labels=test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review\n"
     ]
    },
    {
     "data": {
      "text/plain": "'what a great word  re  imagining  is . isn  t that what they call dawn of the dead mmiv        a clever word indeed  it disguises the term that everyone has grown to hate   remake  that is  and makes it almost sound as if the process of making one was creative and involved the imagination . well  damn  was i misled . at least i was seduced more by the thought of countless gore and unbridled violence than by the idea of  re  imagining   though it played a role .  br    br   still  why make a remake  directors do it for only a few reasons really to update a movie for a modern audience  or because they personally love the original and want to make a tribute to it . an homage  if you will . nonetheless  it all generally  i do admit exceptions  boils down to one thing stealing someone  s idea and reshaping it  or  re  imagining  it  so that those who would never see it or understand it would pay money to see it . it  s like coles   cliffs  notes dump everything in a blender  purify all that is more puzzling and curious and throw in a few artificial flavors . in other words  a great marketing scheme .  br    br   so what  s wrong with this one  well  i  ll start with what i liked . i liked the opening scenes . thanks to cgi and a bigger budget we could actually get a grasp of the chaos of the zombie holocaust romero tried to communicate in the original through minimalist means . we see the city in ruins  thousands of zombies chaos and death . two words that look beautiful on screen . then it all falls apart .  br    br   this set  up leads nowhere . the movie does what almost every remake does . it adds more of everything except character  atmosphere  and story . it  s noisier   in some sense  bloodier  and more full of main characters who appear only to die in nonsensical subplots . the setting  the mall which played a crucial role in the original film  s story and theme  is purely coincidental . the idea communicated in romero  s film  the pure ecstatic joy of having  a mall all to yourself as a fortress   is gone here . further  this  re  imagining  has no moxie  no spirit  no balls . it assumes  probably quite rightly  that the audience has no attention span and doesn  t bother to get us interested in the characters or the story . the film is rushed and misses the quieter interactions of the four characters of the original . you actually grew to care about those people in romero  s version because there was a certain realism to their existence despite the insanity outside the mall . here  you don  t care when or who goes what matters is how they go .  br    br   what else is their to say  the film is not scary . it has one or two  jump  scenes and it tries to make up for the rest with gore and loud special effects . as a story it  s really too choppy to be followed and the conflicts between the characters are too underdeveloped to save it . the humor is also reduced to a few one  liners  and one really good character andy  . after that  what remains  an ending that is plainly ridiculous and far inferior to the subdued  inevitable ambiguity of the original film . but  despite it being a pretty bad film  though not quite as bad as some other remakes   it should be remembered for one thing it kicked the passion of christ from it  s number one spot in the box office . well done zombies .  '"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model output 0.12929830940792372\n",
      "predicted negative\n",
      "actual negative\n"
     ]
    }
   ],
   "source": [
    "idx=np.random.randint(0,len(reviews))\n",
    "print(\"Review\")\n",
    "display(reviews[idx])\n",
    "output=model(reviews[idx])\n",
    "predicted=\"positive\"\n",
    "if output<0.5:\n",
    "    predicted=\"negative\"\n",
    "print(f\"model output {output[0][0]}\")\n",
    "print(f\"predicted {predicted}\")\n",
    "print(f\"actual {labels[idx]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}