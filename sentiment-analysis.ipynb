{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import helper\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "from IPython.display import display\n",
    "import time\n",
    "import sys\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Load and view Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "yes its an art . . . to successfully make a slow paced thriller .  br    br   the story unfolds in nice volumes while you don  t even notice it happening .  br    br   fine performance by robin williams . the sexuality angles in the film can seem unnecessary and can probably affect how much you enjoy the film . however  the core plot is very engaging . the movie doesn  t rush onto you and still grips you enough to keep you wondering . the direction is good . use of lights to achieve desired affects of suspense and unexpectedness is good .  br    br   very nice  time watch if you are looking to lay back and hear a thrilling short story   \n",
      "positive\n"
     ]
    }
   ],
   "source": [
    "reviews, labels = helper.load_data()\n",
    "print(reviews[12])\n",
    "print(labels[38])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# understand dataset\n",
    "- in this part i will try to find what is the reason for a review to be positive and what is the words that appear in positive or negative review\n",
    "- we want to know which words appear in negative and positive\n",
    "- positive words and negative_words are counter objects will have count of each word exist either in negative reviews or positive reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no of positive reviews=12500\n",
      "no of negative reviews=12500\n"
     ]
    }
   ],
   "source": [
    "no_positive_rev = labels.count('positive')\n",
    "no_negative_rev = len(labels) - no_positive_rev\n",
    "print(f\"no of positive reviews={no_positive_rev}\")\n",
    "print(f\"no of negative reviews={no_negative_rev}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['positive', 'negative', 'positive', 'negative', 'positive', 'negative', 'positive', 'negative', 'positive', 'negative', 'positive', 'negative', 'positive', 'negative', 'positive', 'negative', 'positive', 'negative', 'positive', 'negative', 'positive', 'negative', 'positive', 'negative', 'positive']\n"
     ]
    }
   ],
   "source": [
    "print(labels[:25])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "\n",
    "- **data is evenly distributed data even indices for positive and  odd for negative**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "positive_words_cnt = Counter()\n",
    "negative_words_cnt = Counter()\n",
    "all_words_cnt = Counter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "for i in range(len(reviews)):\n",
    "    words = reviews[i].split(\" \")\n",
    "    for word in words:\n",
    "        if labels[i] == 'positive':\n",
    "            positive_words_cnt[word] += 1\n",
    "        else:\n",
    "            negative_words_cnt[word] += 1\n",
    "        all_words_cnt[word] += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## showing the most common words appear in positive and also in negative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('', 550468),\n",
       " ('the', 173324),\n",
       " ('.', 159654),\n",
       " ('and', 89722),\n",
       " ('a', 83688),\n",
       " ('of', 76855),\n",
       " ('to', 66746),\n",
       " ('is', 57245),\n",
       " ('in', 50215),\n",
       " ('br', 49235),\n",
       " ('it', 48025),\n",
       " ('i', 40743),\n",
       " ('that', 35630),\n",
       " ('this', 35080),\n",
       " ('s', 33815),\n",
       " ('as', 26308),\n",
       " ('with', 23247),\n",
       " ('for', 22416),\n",
       " ('was', 21917),\n",
       " ('film', 20937),\n",
       " ('but', 20822),\n",
       " ('movie', 19074),\n",
       " ('his', 17227),\n",
       " ('on', 17008),\n",
       " ('you', 16681),\n",
       " ('he', 16282),\n",
       " ('are', 14807),\n",
       " ('not', 14272),\n",
       " ('t', 13720),\n",
       " ('one', 13655),\n",
       " ('have', 12587),\n",
       " ('be', 12416),\n",
       " ('by', 11997),\n",
       " ('all', 11942),\n",
       " ('who', 11464),\n",
       " ('an', 11294),\n",
       " ('at', 11234),\n",
       " ('from', 10767),\n",
       " ('her', 10474),\n",
       " ('they', 9895),\n",
       " ('has', 9186),\n",
       " ('so', 9154),\n",
       " ('like', 9038),\n",
       " ('about', 8313),\n",
       " ('very', 8305),\n",
       " ('out', 8134),\n",
       " ('there', 8057),\n",
       " ('she', 7779),\n",
       " ('what', 7737),\n",
       " ('or', 7732),\n",
       " ('good', 7720),\n",
       " ('more', 7521),\n",
       " ('when', 7456),\n",
       " ('some', 7441),\n",
       " ('if', 7285),\n",
       " ('just', 7152),\n",
       " ('can', 7001),\n",
       " ('story', 6780),\n",
       " ('time', 6515),\n",
       " ('my', 6488),\n",
       " ('great', 6419),\n",
       " ('well', 6405),\n",
       " ('up', 6321),\n",
       " ('which', 6267),\n",
       " ('their', 6107),\n",
       " ('see', 6026),\n",
       " ('also', 5550),\n",
       " ('we', 5531),\n",
       " ('really', 5476),\n",
       " ('would', 5400),\n",
       " ('will', 5218),\n",
       " ('me', 5167),\n",
       " ('had', 5148),\n",
       " ('only', 5137),\n",
       " ('him', 5018),\n",
       " ('even', 4964),\n",
       " ('most', 4864),\n",
       " ('other', 4858),\n",
       " ('were', 4782),\n",
       " ('first', 4755),\n",
       " ('than', 4736),\n",
       " ('much', 4685),\n",
       " ('its', 4622),\n",
       " ('no', 4574),\n",
       " ('into', 4544),\n",
       " ('people', 4479),\n",
       " ('best', 4319),\n",
       " ('love', 4301),\n",
       " ('get', 4272),\n",
       " ('how', 4213),\n",
       " ('life', 4199),\n",
       " ('been', 4189),\n",
       " ('because', 4079),\n",
       " ('way', 4036),\n",
       " ('do', 3941),\n",
       " ('made', 3823),\n",
       " ('films', 3813),\n",
       " ('them', 3805),\n",
       " ('after', 3800),\n",
       " ('many', 3766)]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[('', 561462),\n",
       " ('.', 167538),\n",
       " ('the', 163389),\n",
       " ('a', 79321),\n",
       " ('and', 74385),\n",
       " ('of', 69009),\n",
       " ('to', 68974),\n",
       " ('br', 52637),\n",
       " ('is', 50083),\n",
       " ('it', 48327),\n",
       " ('i', 46880),\n",
       " ('in', 43753),\n",
       " ('this', 40920),\n",
       " ('that', 37615),\n",
       " ('s', 31546),\n",
       " ('was', 26291),\n",
       " ('movie', 24965),\n",
       " ('for', 21927),\n",
       " ('but', 21781),\n",
       " ('with', 20878),\n",
       " ('as', 20625),\n",
       " ('t', 20361),\n",
       " ('film', 19218),\n",
       " ('you', 17549),\n",
       " ('on', 17192),\n",
       " ('not', 16354),\n",
       " ('have', 15144),\n",
       " ('are', 14623),\n",
       " ('be', 14541),\n",
       " ('he', 13856),\n",
       " ('one', 13134),\n",
       " ('they', 13011),\n",
       " ('at', 12279),\n",
       " ('his', 12147),\n",
       " ('all', 12036),\n",
       " ('so', 11463),\n",
       " ('like', 11238),\n",
       " ('there', 10775),\n",
       " ('just', 10619),\n",
       " ('by', 10549),\n",
       " ('or', 10272),\n",
       " ('an', 10266),\n",
       " ('who', 9969),\n",
       " ('from', 9731),\n",
       " ('if', 9518),\n",
       " ('about', 9061),\n",
       " ('out', 8979),\n",
       " ('what', 8422),\n",
       " ('some', 8306),\n",
       " ('no', 8143),\n",
       " ('her', 7947),\n",
       " ('even', 7687),\n",
       " ('can', 7653),\n",
       " ('has', 7604),\n",
       " ('good', 7423),\n",
       " ('bad', 7401),\n",
       " ('would', 7036),\n",
       " ('up', 6970),\n",
       " ('only', 6781),\n",
       " ('more', 6730),\n",
       " ('when', 6726),\n",
       " ('she', 6444),\n",
       " ('really', 6262),\n",
       " ('time', 6209),\n",
       " ('had', 6142),\n",
       " ('my', 6015),\n",
       " ('were', 6001),\n",
       " ('which', 5780),\n",
       " ('very', 5764),\n",
       " ('me', 5606),\n",
       " ('see', 5452),\n",
       " ('don', 5336),\n",
       " ('we', 5328),\n",
       " ('their', 5278),\n",
       " ('do', 5236),\n",
       " ('story', 5208),\n",
       " ('than', 5183),\n",
       " ('been', 5100),\n",
       " ('much', 5078),\n",
       " ('get', 5037),\n",
       " ('because', 4966),\n",
       " ('people', 4806),\n",
       " ('then', 4761),\n",
       " ('make', 4722),\n",
       " ('how', 4688),\n",
       " ('could', 4686),\n",
       " ('any', 4658),\n",
       " ('into', 4567),\n",
       " ('made', 4541),\n",
       " ('first', 4306),\n",
       " ('other', 4305),\n",
       " ('well', 4254),\n",
       " ('too', 4174),\n",
       " ('them', 4165),\n",
       " ('plot', 4154),\n",
       " ('movies', 4080),\n",
       " ('acting', 4056),\n",
       " ('will', 3993),\n",
       " ('way', 3989),\n",
       " ('most', 3919)]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(positive_words_cnt.most_common(100))\n",
    "display(negative_words_cnt.most_common(100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- we have now the count of each word in both positive and negative, but we want the words that appear in positive only and doesn't appear in negative and vice versa\n",
    "- away for this to calculate the ration of the word log(positive_cnt/negative_cnt) if positive~negative then it's value will be ~0 if it appears in positive it will be large number greater than 0 and for negative it will be large negative number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "words_pos_neg_ratio = Counter()\n",
    "for word, cnt in all_words_cnt.items():\n",
    "    if cnt < 100:  #doesn't appear much\n",
    "        continue\n",
    "    words_pos_neg_ratio[word] = np.log((positive_words_cnt[word] + 1) / (negative_words_cnt[word] + 1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "common words\n",
      "pos and negative ratio for 'the' 0.05902846378582202\n",
      "pos and negative ratio for 'is' 0.13365617519106257\n",
      "\n",
      "Postive words\n",
      "pos and negative ratio for 'wonderful' 1.56527119000835\n",
      "pos and negative ratio for 'wonderful' 1.3929263134236418\n",
      "\n",
      "negative words\n",
      "pos and negative ratio for 'bad' -1.3556946609378695\n",
      "pos and negative ratio for 'bad' -1.7327361287785938\n"
     ]
    }
   ],
   "source": [
    "print(\"common words\")\n",
    "print(f\"pos and negative ratio for 'the' {words_pos_neg_ratio['the']}\")\n",
    "print(f\"pos and negative ratio for 'is' {words_pos_neg_ratio['is']}\")\n",
    "print(\"\\nPostive words\")\n",
    "\n",
    "print(f\"pos and negative ratio for 'wonderful' {words_pos_neg_ratio['wonderful']}\")\n",
    "print(f\"pos and negative ratio for 'wonderful' {words_pos_neg_ratio['amazing']}\")\n",
    "\n",
    "print(\"\\nnegative words\")\n",
    "print(f\"pos and negative ratio for 'bad' {words_pos_neg_ratio['bad']}\")\n",
    "print(f\"pos and negative ratio for 'bad' {words_pos_neg_ratio['worse']}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "common words ~ 0\n",
    "positive words >1\n",
    "negative words <-1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positive most common\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('edie', 4.700480365792417),\n",
       " ('paulie', 4.085976312551584),\n",
       " ('felix', 3.1612467120315646),\n",
       " ('polanski', 2.833213344056216),\n",
       " ('matthau', 2.8134107167600364),\n",
       " ('victoria', 2.6855773452501515),\n",
       " ('mildred', 2.6119063405493077),\n",
       " ('gandhi', 2.5477075510270306),\n",
       " ('flawless', 2.4595888418037104),\n",
       " ('superbly', 2.268683541318364),\n",
       " ('perfection', 2.1671471220989416),\n",
       " ('astaire', 2.1484344131667874),\n",
       " ('captures', 2.0438143640366846),\n",
       " ('voight', 2.0402208285265546),\n",
       " ('wonderfully', 2.02537432040956),\n",
       " ('powell', 1.9836504770381602),\n",
       " ('brosnan', 1.9636097261547143),\n",
       " ('lily', 1.9289605907415401),\n",
       " ('bakshi', 1.911718784307034),\n",
       " ('lincoln', 1.9079309009900969),\n",
       " ('refreshing', 1.8607523407150064),\n",
       " ('lemmon', 1.8562979903656263),\n",
       " ('breathtaking', 1.8549383708495866),\n",
       " ('bourne', 1.8538912503350613),\n",
       " ('flynn', 1.807507826196194),\n",
       " ('delightful', 1.8044984950054848),\n",
       " ('andrews', 1.7841548698428356),\n",
       " ('homer', 1.7805861686299298),\n",
       " ('soccer', 1.7692866133759964),\n",
       " ('beautifully', 1.76537271405486),\n",
       " ('lumet', 1.7578579175523736),\n",
       " ('elvira', 1.7473077066572211),\n",
       " ('underrated', 1.7247487589450947),\n",
       " ('gripping', 1.7243181884325225),\n",
       " ('superb', 1.71090737259896),\n",
       " ('delight', 1.6789639750827108),\n",
       " ('sadness', 1.6739764335716716),\n",
       " ('welles', 1.6724127115954888),\n",
       " ('sinatra', 1.6438393391514328),\n",
       " ('touching', 1.6399534563600509),\n",
       " ('timeless', 1.6389967146756448),\n",
       " ('macy', 1.6326947745983675),\n",
       " ('unforgettable', 1.6259672143853108),\n",
       " ('favorites', 1.6222586008631619),\n",
       " ('hartley', 1.6211339521972916),\n",
       " ('extraordinary', 1.6163107917218624),\n",
       " ('sullivan', 1.6156684621847364),\n",
       " ('stewart', 1.6145530131008707),\n",
       " ('brilliantly', 1.5998684614179497),\n",
       " ('friendship', 1.5720115069149834),\n",
       " ('palma', 1.5664205273504097),\n",
       " ('wonderful', 1.56527119000835),\n",
       " ('magnificent', 1.5512559570513647),\n",
       " ('finest', 1.550597412411167),\n",
       " ('ritter', 1.5493339883643948),\n",
       " ('jackie', 1.5491181222005777),\n",
       " ('tremendous', 1.5279448781829175),\n",
       " ('freedom', 1.5141277326297755),\n",
       " ('fantastic', 1.5063736090366242),\n",
       " ('terrific', 1.505482878385009),\n",
       " ('sidney', 1.5007047122976347),\n",
       " ('pleasantly', 1.4992347723004862),\n",
       " ('mann', 1.4992347723004862),\n",
       " ('noir', 1.4968362355197145),\n",
       " ('outstanding', 1.493925025312256),\n",
       " ('nancy', 1.4934389985712184),\n",
       " ('marie', 1.4873904779912595),\n",
       " ('marvelous', 1.4816045409242156),\n",
       " ('ruth', 1.4696759700589417),\n",
       " ('excellent', 1.4653478511838256),\n",
       " ('stanwyck', 1.4488147181012245),\n",
       " ('widmark', 1.442989704796436),\n",
       " ('splendid', 1.4370666864933137),\n",
       " ('chan', 1.4291143583028187),\n",
       " ('exceptional', 1.4284947156102672),\n",
       " ('tender', 1.423108334242607),\n",
       " ('gentle', 1.418382675671391),\n",
       " ('poignant', 1.410199881973445),\n",
       " ('gem', 1.3966571481554373),\n",
       " ('captivating', 1.3966571481554373),\n",
       " ('fisher', 1.3958638121360414),\n",
       " ('davies', 1.3958638121360414),\n",
       " ('chilling', 1.3936204012119635),\n",
       " ('amazing', 1.3929263134236418),\n",
       " ('darker', 1.3758230612525952),\n",
       " ('april', 1.3621968095408301),\n",
       " ('blake', 1.3531421538029902),\n",
       " ('kelly', 1.3490940774338),\n",
       " ('overlooked', 1.3388921222253067),\n",
       " ('ralph', 1.3366974199805186),\n",
       " ('bette', 1.3237740041385566),\n",
       " ('hoffman', 1.3217558399823195),\n",
       " ('cole', 1.3217558399823195),\n",
       " ('shines', 1.3133875903118029),\n",
       " ('powerful', 1.3020133612531182),\n",
       " ('notch', 1.3013949173334043),\n",
       " ('winters', 1.295322582914164),\n",
       " ('pitt', 1.292768303109067),\n",
       " ('remarkable', 1.2924756059022358),\n",
       " ('vivid', 1.2878542883066382)]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"Positive most common\")\n",
    "display(words_pos_neg_ratio.most_common(100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "words like magnificent and amazing, wonderful, appear from top 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Negative most common\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('boll', -4.276666119016055),\n",
       " ('uwe', -3.9318256327243257),\n",
       " ('seagal', -3.4210000089583352),\n",
       " ('unwatchable', -3.0349529867072724),\n",
       " ('stinker', -2.9856819377004897),\n",
       " ('mst', -2.8449093838194073),\n",
       " ('incoherent', -2.803360380906535),\n",
       " ('unfunny', -2.635081181235619),\n",
       " ('waste', -2.6093342281630525),\n",
       " ('blah', -2.501435951739211),\n",
       " ('pointless', -2.430613567421338),\n",
       " ('horrid', -2.379546134130174),\n",
       " ('atrocious', -2.36528368720961),\n",
       " ('redeeming', -2.3331477434042123),\n",
       " ('worst', -2.283027494964281),\n",
       " ('prom', -2.2655438213136967),\n",
       " ('drivel', -2.26002547857525),\n",
       " ('lousy', -2.2587824703356527),\n",
       " ('laughable', -2.2396712675834767),\n",
       " ('awful', -2.22125951150762),\n",
       " ('poorly', -2.206570439754457),\n",
       " ('remotely', -2.145931282948669),\n",
       " ('wasting', -2.1400661634962708),\n",
       " ('existent', -2.02537432040956),\n",
       " ('lame', -1.970717622759581),\n",
       " ('sucks', -1.9580806846755685),\n",
       " ('insult', -1.94060509682562),\n",
       " ('boredom', -1.9379419794061366),\n",
       " ('miserably', -1.927891643552635),\n",
       " ('uninspired', -1.9187591599893623),\n",
       " ('uninteresting', -1.9066894359020319),\n",
       " ('horrible', -1.9038282036209997),\n",
       " ('pathetic', -1.8841327893590702),\n",
       " ('godzilla', -1.8827312474337816),\n",
       " ('unconvincing', -1.8748743759385615),\n",
       " ('amateurish', -1.8744511850731684),\n",
       " ('appalling', -1.8672670217362002),\n",
       " ('gadget', -1.8666607774011728),\n",
       " ('idiotic', -1.8484548129046003),\n",
       " ('unintentional', -1.845826690498331),\n",
       " ('stupidity', -1.836211231798889),\n",
       " ('wasted', -1.82537608002704),\n",
       " ('crap', -1.821206670554356),\n",
       " ('cardboard', -1.791759469228055),\n",
       " ('tedious', -1.7707060600302227),\n",
       " ('insulting', -1.7650912221458936),\n",
       " ('dreadful', -1.7443572303334711),\n",
       " ('badly', -1.743474228068593),\n",
       " ('worse', -1.7327361287785938),\n",
       " ('terrible', -1.7250680947293828),\n",
       " ('suck', -1.7147984280919266),\n",
       " ('dire', -1.709521370991083),\n",
       " ('mess', -1.6801454845983868),\n",
       " ('embarrassing', -1.6739764335716716),\n",
       " ('garbage', -1.6731199024700552),\n",
       " ('stupid', -1.6516020715417972),\n",
       " ('pile', -1.6384254493073527),\n",
       " ('vampires', -1.5950491749820006),\n",
       " ('ashamed', -1.5869650565820417),\n",
       " ('dull', -1.5775048653310912),\n",
       " ('worthless', -1.5723966407537513),\n",
       " ('avoid', -1.568304924286708),\n",
       " ('wooden', -1.552685095841651),\n",
       " ('inept', -1.5496194172231903),\n",
       " ('forgettable', -1.5248805244060373),\n",
       " ('crappy', -1.5141277326297755),\n",
       " ('ridiculous', -1.502813972729502),\n",
       " ('bat', -1.5004867286455454),\n",
       " ('fulci', -1.498772344546581),\n",
       " ('excuse', -1.4968362355197145),\n",
       " ('whatsoever', -1.494616588272045),\n",
       " ('rubbish', -1.48870936654796),\n",
       " ('boring', -1.4879804697340995),\n",
       " ('unbelievably', -1.4733057381095205),\n",
       " ('junk', -1.466337068793427),\n",
       " ('turkey', -1.4593194961347804),\n",
       " ('shark', -1.449095262358921),\n",
       " ('flop', -1.4441139320087168),\n",
       " ('topless', -1.4350845252893227),\n",
       " ('useless', -1.4350845252893227),\n",
       " ('ripped', -1.4315509527080115),\n",
       " ('ridiculously', -1.4307461236907244),\n",
       " ('embarrassed', -1.4246132254220272),\n",
       " ('seed', -1.423108334242607),\n",
       " ('costs', -1.418382675671391),\n",
       " ('dumb', -1.415281897993143),\n",
       " ('bother', -1.4116121691041805),\n",
       " ('rambo', -1.3971052772241062),\n",
       " ('horrendous', -1.3955110162248145),\n",
       " ('horribly', -1.3920914788042167),\n",
       " ('plastic', -1.3862943611198906),\n",
       " ('hideous', -1.3862943611198906),\n",
       " ('fest', -1.3773256911371303),\n",
       " ('disjointed', -1.374318170073175),\n",
       " ('ludicrous', -1.3723081191451507),\n",
       " ('bland', -1.3682758556172123),\n",
       " ('annoying', -1.3614791920001665),\n",
       " ('unintentionally', -1.3591433720539396),\n",
       " ('mildly', -1.3581234841531944),\n",
       " ('obnoxious', -1.3564413979702095)]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"Negative most common\")\n",
    "display(list(reversed(words_pos_neg_ratio.most_common()))[0:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max value:4.700480365792417\n",
      "min value:-4.276666119016055\n",
      "Words count:74074\n"
     ]
    }
   ],
   "source": [
    "print(f\"max value:{helper.max_counter_value(words_pos_neg_ratio)}\")\n",
    "print(f\"min value:{helper.min_counter_value(words_pos_neg_ratio)}\")\n",
    "print(f\"Words count:{len(all_words_cnt)}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Creating the Model\n",
    "- ***Network structure***\n",
    "![network](notebook-images/network.png)\n",
    "- ---\n",
    "- after viewing the data existence of some words in a review could classify it to positive or negative so our network will be as follows\n",
    "- we need to represent the data input as vector of zeros and ones when the word exists in a review it will have value one in the index of the word in the vector\n",
    "- we need to ignore irrelevant words that appear in both positive and negative via threshold in pos_negative_ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class Model:\n",
    "    def __init__(self, reviews, labels, hidden_nodes=128, threshold=0.2,load_weights=False):\n",
    "        np.random.seed(69)\n",
    "        self.hidden_nodes = hidden_nodes\n",
    "        self.threshold = threshold\n",
    "\n",
    "        filtered_vocab = self.data_preprocessing(reviews, labels)\n",
    "        self.word2idx = {}\n",
    "        for idx, word in enumerate(filtered_vocab):\n",
    "            self.word2idx[word] = idx\n",
    "        self.input_size = len(filtered_vocab)\n",
    "        self.fc1 = helper.NNLiner(self.input_size, self.hidden_nodes)\n",
    "        # self.fc1.weights=np.zeros((self.input_size,self.hidden_nodes))\n",
    "        self.fc2 = helper.NNLiner(self.hidden_nodes, 1)\n",
    "\n",
    "        self.sigmoid = lambda x: 1.0 / (1.0 + (np.exp(-x)))\n",
    "\n",
    "        if load_weights:\n",
    "            try:\n",
    "                self.load_weights()\n",
    "                print(\"model loaded with saved weights\")\n",
    "            except:\n",
    "                print(\"model loaded with random weights\")\n",
    "\n",
    "    def review_encode(self, review):\n",
    "        a0 = np.zeros(self.input_size, float)\n",
    "        for word in review.split(' '):\n",
    "            if word in self.word2idx:\n",
    "                a0[self.word2idx[word]] = 1.0\n",
    "        return np.expand_dims(a0, axis=0)\n",
    "\n",
    "    def train(self,epochs,learning_rate, train_reviews, train_labels,test_reviews,test_labels):\n",
    "\n",
    "        assert (len(train_reviews) == len(train_labels))\n",
    "        for e in range(epochs):\n",
    "            data_size=len(train_labels)\n",
    "            correct_cnt = 0\n",
    "            forward_cnt = 0\n",
    "            print_every=int(len(train_labels)*0.1) #print log every 10 percent\n",
    "            start_time = time.time()\n",
    "            train_accuracy=0\n",
    "            print(f\"\\nepoch {e+1}\\n\")\n",
    "            for review, label in zip(train_reviews, train_labels):\n",
    "                forward_cnt += 1\n",
    "                if label == 'positive':\n",
    "                    actual_output = 1\n",
    "                else:\n",
    "                    actual_output = 0\n",
    "\n",
    "                #------------------------------Feed-Forward----------------------------------------#\n",
    "                a0 = self.review_encode(review)\n",
    "                z1 = self.fc1(a0)\n",
    "                # a1=self.sigmoid(z1)\n",
    "                a1 = z1\n",
    "\n",
    "                z2 = self.fc2(a1)\n",
    "                a2 = self.sigmoid(z2)\n",
    "\n",
    "                predicted = 1\n",
    "                if a2 < 0.5:\n",
    "                    predicted = 0\n",
    "                if predicted == actual_output:\n",
    "                    correct_cnt += 1\n",
    "                #d --> derivative\n",
    "\n",
    "                error = (1 / 2) * (actual_output - a2) ** 2 # 2d matrix [[error]]\n",
    "                error=error[0][0]\n",
    "                #______________________________Backpropagation_______________________________________#\n",
    "                #Ti --> error term of layer i\n",
    "                #Ti+1=Ti @\n",
    "\n",
    "                # _______________delta_w2____________________\n",
    "                #         |  error term 2  |\n",
    "                # dE/dW2 = dE/a2 * da2/dz2 * dz2/dw2 = T2 * da2/dz2\n",
    "                de_da2 = -(actual_output - a2)\n",
    "                da2_dz2 = a2 * (1 - a2)  # derivative of sigmoid is sigmoid*(1-sigmoid)\n",
    "\n",
    "                t2 = de_da2 * da2_dz2  # --> dimensions (1x2) = dimensions of a2\n",
    "\n",
    "                de_dw2 = (a1.transpose()) @ t2  #--> (3*1)@ (1*2) = (3x2) same like w2 size\n",
    "                #-----------------------------------------------\n",
    "\n",
    "                # _______________delta_w2____________________\n",
    "                #dE/dW1=dE/a2 * da2/dz2 * dz2/da1 * da1/dz1 * dz1/dw1 = t2 * dz2/da1 * da1/dz1 * dz1/dw1\n",
    "\n",
    "                #dz2/da1= d/da1 (a1@W2 + b2) = W2\n",
    "                # da1_z1=a1*(1-a1)\n",
    "                da1_z1 = 1\n",
    "                #dz1/dw1=a0\n",
    "\n",
    "                t1 = t2 @ self.fc2.weights.T * da1_z1\n",
    "                de_dw1 = a0.transpose() @ t1\n",
    "\n",
    "                #------------ update weights ------------------#\n",
    "                self.fc1.weights -= (de_dw1 * learning_rate)\n",
    "                self.fc2.weights -= (de_dw2 * learning_rate)\n",
    "                #-----------------------------------------------\n",
    "\n",
    "                elapsed_time = float(time.time() - start_time)\n",
    "                reviews_per_second = forward_cnt / elapsed_time if elapsed_time > 0 else 0\n",
    "\n",
    "                train_accuracy=(correct_cnt * 100) / float(forward_cnt + 1)\n",
    "\n",
    "                sys.stdout.write(\"\\rProgress:\" + str(100 * forward_cnt / float(data_size))[:4]\n",
    "                                 + \"% Speed(reviews/sec):\" + str(reviews_per_second)[0:5]\n",
    "                                 + \" #Correct:\" + str(correct_cnt) + \" #Trained:\" + str(forward_cnt + 1)\n",
    "                                 + \" Training Accuracy:\" + str(train_accuracy)[:4] + \"%\"+\n",
    "                                 \" Train loss: \"+str(error)[:8])\n",
    "                if forward_cnt % print_every == 0:\n",
    "                    print(\"\")\n",
    "            print(\"\\nTesting\\n\")\n",
    "            #------------------------------------------------------Testing----------------------------------------------------------#\n",
    "            correct_cnt = 0\n",
    "            forward_cnt = 0\n",
    "            start_time = time.time()\n",
    "            data_size=len(test_labels)\n",
    "            print_every=int(len(test_labels)*0.1) #print log every 10 percent\n",
    "            test_accuracy=0\n",
    "\n",
    "            for review, label in zip(test_reviews, test_labels):\n",
    "                forward_cnt += 1\n",
    "                if label == 'positive':\n",
    "                    actual_output = 1\n",
    "                else:\n",
    "                    actual_output = 0\n",
    "                #------------------------------Feed-Forward----------------------------------------#\n",
    "                a2=self(review)\n",
    "                error = (1 / 2) * (actual_output - a2) ** 2 # 2d matrix [[error]]\n",
    "                error=error[0][0]\n",
    "\n",
    "                predicted = 1\n",
    "                if a2 < 0.5:\n",
    "                    predicted = 0\n",
    "                if predicted == actual_output:\n",
    "                    correct_cnt += 1\n",
    "                elapsed_time = float(time.time() - start_time)\n",
    "                reviews_per_second = forward_cnt / elapsed_time if elapsed_time > 0 else 0\n",
    "                test_accuracy=(correct_cnt * 100) / float(forward_cnt + 1)\n",
    "\n",
    "\n",
    "                sys.stdout.write(\"\\rProgress:\" + str(100 * forward_cnt / float(data_size))[:4]\n",
    "                                 + \"% Speed(reviews/sec):\" + str(reviews_per_second)[0:5]\n",
    "                                 + \" #Correct:\" + str(correct_cnt) + \" #Tested:\" + str(forward_cnt + 1)\n",
    "                                 + \" Testing Accuracy:\" + str(test_accuracy)[:4] + \"%\"+\n",
    "                                 \" Test loss: \"+str(error)[:8])\n",
    "                if forward_cnt % print_every == 0:\n",
    "                    print(\"\")\n",
    "            if train_accuracy>test_accuracy:\n",
    "                print(\"\\nWarning overfitting training loop will break\")\n",
    "                break\n",
    "            else :\n",
    "                self.save_weights()\n",
    "    def save_weights(self):\n",
    "        np.save('model_weights/fc1.npy',self.fc1.weights)\n",
    "        np.save('model_weights/fc2.npy',self.fc2.weights)\n",
    "    def load_weights(self):\n",
    "        try:\n",
    "            self.fc1.weights=np.load('model_weights/fc1.npy')\n",
    "        except FileNotFoundError:\n",
    "            raise Exception(f\"can't load fc1 weights please check that fc1 weights exists in 'model_weights/fc1.npy' \")\n",
    "        try:\n",
    "            self.fc2.weights=np.load('model_weights/fc2.npy')\n",
    "        except FileNotFoundError:\n",
    "            raise Exception(f\"can't load fc2 weights please check that fc2 weights exists in 'model_weights/fc2.npy' \")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def data_preprocessing(self, reviews, labels):\n",
    "        # calculate the importance of each word using pos\n",
    "        filtered_reviews_vocab = set()\n",
    "        positive_words_cnt = Counter()\n",
    "        negative_words_cnt = Counter()\n",
    "        reviews_vocab = set()\n",
    "\n",
    "        for i in range(len(reviews)):\n",
    "            words = reviews[i].split(' ')\n",
    "            for word in words:\n",
    "                if labels[i] == 'positive':\n",
    "                    positive_words_cnt[word] += 1\n",
    "                else:\n",
    "                    negative_words_cnt[word] += 1\n",
    "                reviews_vocab.add(word)\n",
    "\n",
    "        for word in reviews_vocab:\n",
    "            word_ratio = np.log((positive_words_cnt[word] + 1) / (negative_words_cnt[word] + 1))\n",
    "            if abs(word_ratio) >= self.threshold:\n",
    "                filtered_reviews_vocab.add(word)\n",
    "        return filtered_reviews_vocab\n",
    "\n",
    "    def __call__(self, review):\n",
    "        a0 = self.review_encode(review)\n",
    "\n",
    "        z1 = self.fc1(a0)\n",
    "        # a1 = self.sigmoid(z1)\n",
    "        a1=z1\n",
    "        z2 = self.fc2(a1)\n",
    "        a2 = self.sigmoid(z2)\n",
    "        return a2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "model = Model(reviews, labels, hidden_nodes=25, threshold=0.15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.49996031]]\n",
      "positive\n"
     ]
    }
   ],
   "source": [
    "x = 0\n",
    "print(model.__call__(reviews[x]))\n",
    "print(labels[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total records=25000\n",
      "train_size=18750\n",
      "test_size=6250\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "test_percentage = 0.25\n",
    "\n",
    "train_reviews,test_reviews,train_labels,test_labels=train_test_split(reviews,labels,test_size=test_percentage,random_state=69)\n",
    "data_size = int(len(reviews))\n",
    "\n",
    "print(f\"total records={data_size}\\ntrain_size={len(train_reviews)}\\ntest_size={len(test_reviews)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "epoch 1\n",
      "\n",
      "Progress:10.0% Speed(reviews/sec):38.29 #Correct:1351 #Trained:1876 Training Accuracy:72.0% Train loss: 0.074717\n",
      "Progress:20.0% Speed(reviews/sec):39.91 #Correct:2869 #Trained:3751 Training Accuracy:76.4% Train loss: 0.098566\n",
      "Progress:30.0% Speed(reviews/sec):40.48 #Correct:4430 #Trained:5626 Training Accuracy:78.7% Train loss: 0.008967\n",
      "Progress:40.0% Speed(reviews/sec):40.77 #Correct:6016 #Trained:7501 Training Accuracy:80.2% Train loss: 0.070556\n",
      "Progress:50.0% Speed(reviews/sec):41.16 #Correct:7615 #Trained:9376 Training Accuracy:81.2% Train loss: 0.137546\n",
      "Progress:60.0% Speed(reviews/sec):41.21 #Correct:9233 #Trained:11251 Training Accuracy:82.0% Train loss: 0.000228\n",
      "Progress:70.0% Speed(reviews/sec):41.26 #Correct:10854 #Trained:13126 Training Accuracy:82.6% Train loss: 0.052634\n",
      "Progress:80.0% Speed(reviews/sec):41.34 #Correct:12465 #Trained:15001 Training Accuracy:83.0% Train loss: 0.303897\n",
      "Progress:90.0% Speed(reviews/sec):41.39 #Correct:14087 #Trained:16876 Training Accuracy:83.4% Train loss: 0.000369\n",
      "Progress:100.% Speed(reviews/sec):41.26 #Correct:15694 #Trained:18751 Training Accuracy:83.6% Train loss: 0.000113\n",
      "\n",
      "Testing\n",
      "\n",
      "Progress:10.0% Speed(reviews/sec):227.5 #Correct:555 #Tested:626 Testing Accuracy:88.6% Test loss: 0.002977\n",
      "Progress:20.0% Speed(reviews/sec):229.3 #Correct:1099 #Tested:1251 Testing Accuracy:87.8% Test loss: 0.028431\n",
      "Progress:30.0% Speed(reviews/sec):229.1 #Correct:1642 #Tested:1876 Testing Accuracy:87.5% Test loss: 7.662746\n",
      "Progress:40.0% Speed(reviews/sec):230.5 #Correct:2183 #Tested:2501 Testing Accuracy:87.2% Test loss: 2.283376\n",
      "Progress:50.0% Speed(reviews/sec):230.9 #Correct:2732 #Tested:3126 Testing Accuracy:87.3% Test loss: 0.009941\n",
      "Progress:60.0% Speed(reviews/sec):230.7 #Correct:3295 #Tested:3751 Testing Accuracy:87.8% Test loss: 1.082386\n",
      "Progress:70.0% Speed(reviews/sec):230.3 #Correct:3847 #Tested:4376 Testing Accuracy:87.9% Test loss: 8.967498\n",
      "Progress:80.0% Speed(reviews/sec):221.5 #Correct:4401 #Tested:5001 Testing Accuracy:88.0% Test loss: 1.153918\n",
      "Progress:90.0% Speed(reviews/sec):219.1 #Correct:4957 #Tested:5626 Testing Accuracy:88.1% Test loss: 0.106248\n",
      "Progress:100.% Speed(reviews/sec):219.7 #Correct:5516 #Tested:6251 Testing Accuracy:88.2% Test loss: 0.000209\n",
      "\n",
      "epoch 2\n",
      "\n",
      "Progress:10.0% Speed(reviews/sec):42.86 #Correct:1640 #Trained:1876 Training Accuracy:87.4% Train loss: 0.017268\n",
      "Progress:20.0% Speed(reviews/sec):42.32 #Correct:3318 #Trained:3751 Training Accuracy:88.4% Train loss: 0.059006\n",
      "Progress:30.0% Speed(reviews/sec):42.36 #Correct:5008 #Trained:5626 Training Accuracy:89.0% Train loss: 0.000372\n",
      "Progress:40.0% Speed(reviews/sec):42.30 #Correct:6676 #Trained:7501 Training Accuracy:89.0% Train loss: 0.068649\n",
      "Progress:50.0% Speed(reviews/sec):42.49 #Correct:8372 #Trained:9376 Training Accuracy:89.2% Train loss: 0.082977\n",
      "Progress:60.0% Speed(reviews/sec):41.95 #Correct:10072 #Trained:11251 Training Accuracy:89.5% Train loss: 2.391142\n",
      "Progress:70.0% Speed(reviews/sec):41.46 #Correct:11766 #Trained:13126 Training Accuracy:89.6% Train loss: 0.014120\n",
      "Progress:80.0% Speed(reviews/sec):41.23 #Correct:13454 #Trained:15001 Training Accuracy:89.6% Train loss: 0.323614\n",
      "Progress:90.0% Speed(reviews/sec):41.20 #Correct:15154 #Trained:16876 Training Accuracy:89.7% Train loss: 7.239676\n",
      "Progress:100.% Speed(reviews/sec):41.17 #Correct:16840 #Trained:18751 Training Accuracy:89.8% Train loss: 8.769106\n",
      "\n",
      "Testing\n",
      "\n",
      "Progress:10.0% Speed(reviews/sec):212.8 #Correct:552 #Tested:626 Testing Accuracy:88.1% Test loss: 0.000525\n",
      "Progress:20.0% Speed(reviews/sec):213.0 #Correct:1106 #Tested:1251 Testing Accuracy:88.4% Test loss: 0.031243\n",
      "Progress:30.0% Speed(reviews/sec):212.2 #Correct:1652 #Tested:1876 Testing Accuracy:88.0% Test loss: 7.265176\n",
      "Progress:40.0% Speed(reviews/sec):212.2 #Correct:2196 #Tested:2501 Testing Accuracy:87.8% Test loss: 6.009675\n",
      "Progress:50.0% Speed(reviews/sec):212.2 #Correct:2748 #Tested:3126 Testing Accuracy:87.9% Test loss: 0.001465\n",
      "Progress:60.0% Speed(reviews/sec):210.5 #Correct:3314 #Tested:3751 Testing Accuracy:88.3% Test loss: 7.197493\n",
      "Progress:70.0% Speed(reviews/sec):211.1 #Correct:3869 #Tested:4376 Testing Accuracy:88.4% Test loss: 2.123907\n",
      "Progress:80.0% Speed(reviews/sec):209.5 #Correct:4428 #Tested:5001 Testing Accuracy:88.5% Test loss: 3.522475\n",
      "Progress:90.0% Speed(reviews/sec):206.6 #Correct:4988 #Tested:5626 Testing Accuracy:88.6% Test loss: 0.072018\n",
      "Progress:100.% Speed(reviews/sec):204.2 #Correct:5549 #Tested:6251 Testing Accuracy:88.7% Test loss: 0.000279\n",
      "\n",
      "Warning overfitting training loop will break\n"
     ]
    }
   ],
   "source": [
    "model.train(epochs=5,learning_rate=0.01, train_reviews=train_reviews, train_labels=train_labels,test_reviews=test_reviews,test_labels=test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review\n"
     ]
    },
    {
     "data": {
      "text/plain": "'lordi was a major hype and revelation in     because they won the eurovision song contest with a  not  so  heavy  metal song called  hard rock hallelujah  and appeared on stage dressed like hideous monsters . but  let  s face it  their victory most likely had very little to do with their great musical talents . the eurovision contest gradually turned into one big political circus over the years and lordi probably just won because their song finally brought a little change and  even more importantly  because their whole act sort of ingeniously spoofed the whole annual event . the absolute last thing lordi  s first  and hopefully last  horror film brings is change and ingenuity .  dark floors   based on an idea of the lead singer and starring the rest of the band in supportive roles  is a truly unimaginative and hopeless accumulation of clichs . the immense budget   dark floors  supposedly is the most expensive finnish film ever  definitely assures greatly macabre set pieces and impressive make  up art  but what  s the point where there  s no story that is worth telling  the film takes is set in a busy hospital where a bunch of people  among them a father and his young daughter with an unidentifiable illness  become trapped in the elevator during a power breakdown . when the doors open again  the floors are empty and it looks as if the hospital lies abandoned since many years already . trying to reach the exit  the group stumbles upon several morbid and inexplicable obstacles  like eyeless corpses  screaming ghosts and heavy metal monsters emerging from the floors . the only three points i  m handing out to  dark floors  are exclusively intended for the scenery and the adequate tension building during the first half of the film . for as long as the sinister events don  t require an explanation  the atmosphere is quite creepy  but as soon as you realize the explanation will a  be very stupid or b  never come  the wholesome just collapses like an unstable house of cards . lordi  s costumes never really were scary to begin with  except maybe to traditional eurovision fans  and  in combination with a story more reminiscent to asian ghost  horror  they just look downright pathetic and misfit . with all the national myths and truly unique exterior filming locations  i personally always presumed finland  the land of a thousand lakes  would be the ideal breeding ground for potentially horrific horror tales  but i guess that  s another disillusion on my account .  '"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "'\\n'"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model output 0.34843044577214227\n",
      "predicted negative\n",
      "actual negative\n"
     ]
    }
   ],
   "source": [
    "idx=np.random.randint(0,len(reviews))\n",
    "print(\"Review\")\n",
    "display(reviews[idx],'\\n')\n",
    "output=model(reviews[idx])\n",
    "predicted=\"positive\"\n",
    "if output<0.5:\n",
    "    predicted=\"negative\"\n",
    "print(f\"model output {output[0][0]}\")\n",
    "print(f\"predicted {predicted}\")\n",
    "print(f\"actual {labels[idx]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}