{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import helper\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "from IPython.display import display\n",
    "import time\n",
    "import sys\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Load and view Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "yes its an art . . . to successfully make a slow paced thriller .  br    br   the story unfolds in nice volumes while you don  t even notice it happening .  br    br   fine performance by robin williams . the sexuality angles in the film can seem unnecessary and can probably affect how much you enjoy the film . however  the core plot is very engaging . the movie doesn  t rush onto you and still grips you enough to keep you wondering . the direction is good . use of lights to achieve desired affects of suspense and unexpectedness is good .  br    br   very nice  time watch if you are looking to lay back and hear a thrilling short story   \n",
      "positive\n"
     ]
    }
   ],
   "source": [
    "reviews, labels = helper.load_data()\n",
    "print(reviews[12])\n",
    "print(labels[38])"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# understand dataset\n",
    "- in this part i will try to find what is the reason for a review to be positive and what is the words that appear in positive or negative review\n",
    "- we want to know which words appear in negative and positive\n",
    "- positive words and negative_words are counter objects will have count of each word exist either in negative reviews or positive reviews"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no of positive reviews=12500\n",
      "no of negative reviews=12500\n"
     ]
    }
   ],
   "source": [
    "no_positive_rev = labels.count('positive')\n",
    "no_negative_rev = len(labels) - no_positive_rev\n",
    "print(f\"no of positive reviews={no_positive_rev}\")\n",
    "print(f\"no of negative reviews={no_negative_rev}\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['positive', 'negative', 'positive', 'negative', 'positive', 'negative', 'positive', 'negative', 'positive', 'negative', 'positive', 'negative', 'positive', 'negative', 'positive', 'negative', 'positive', 'negative', 'positive', 'negative', 'positive', 'negative', 'positive', 'negative', 'positive', 'negative', 'positive', 'negative', 'positive', 'negative', 'positive', 'negative', 'positive', 'negative', 'positive', 'negative', 'positive', 'negative', 'positive', 'negative', 'positive', 'negative', 'positive', 'negative', 'positive', 'negative', 'positive', 'negative', 'positive', 'negative', 'positive', 'negative', 'positive', 'negative', 'positive', 'negative', 'positive', 'negative', 'positive', 'negative', 'positive', 'negative', 'positive', 'negative', 'positive', 'negative', 'positive', 'negative', 'positive', 'negative', 'positive', 'negative', 'positive', 'negative', 'positive', 'negative', 'positive', 'negative', 'positive', 'negative', 'positive', 'negative', 'positive', 'negative', 'positive', 'negative', 'positive', 'negative', 'positive', 'negative', 'positive', 'negative', 'positive', 'negative', 'positive', 'negative', 'positive', 'negative', 'positive', 'negative']\n"
     ]
    }
   ],
   "source": [
    "print(labels[:25])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**data is evenly distributed data even indices for positive and  odd for negative**"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "positive_words_cnt = Counter()\n",
    "negative_words_cnt = Counter()\n",
    "all_words_cnt = Counter()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "for i in range(len(reviews)):\n",
    "    words = reviews[i].split(\" \")\n",
    "    for word in words:\n",
    "        if labels[i] == 'positive':\n",
    "            positive_words_cnt[word] += 1\n",
    "        else:\n",
    "            negative_words_cnt[word] += 1\n",
    "        all_words_cnt[word] += 1"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## showing the most common words appear in positive and also in negative"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "outputs": [
    {
     "data": {
      "text/plain": "[('', 550468),\n ('the', 173324),\n ('.', 159654),\n ('and', 89722),\n ('a', 83688),\n ('of', 76855),\n ('to', 66746),\n ('is', 57245),\n ('in', 50215),\n ('br', 49235),\n ('it', 48025),\n ('i', 40743),\n ('that', 35630),\n ('this', 35080),\n ('s', 33815),\n ('as', 26308),\n ('with', 23247),\n ('for', 22416),\n ('was', 21917),\n ('film', 20937),\n ('but', 20822),\n ('movie', 19074),\n ('his', 17227),\n ('on', 17008),\n ('you', 16681),\n ('he', 16282),\n ('are', 14807),\n ('not', 14272),\n ('t', 13720),\n ('one', 13655),\n ('have', 12587),\n ('be', 12416),\n ('by', 11997),\n ('all', 11942),\n ('who', 11464),\n ('an', 11294),\n ('at', 11234),\n ('from', 10767),\n ('her', 10474),\n ('they', 9895),\n ('has', 9186),\n ('so', 9154),\n ('like', 9038),\n ('about', 8313),\n ('very', 8305),\n ('out', 8134),\n ('there', 8057),\n ('she', 7779),\n ('what', 7737),\n ('or', 7732),\n ('good', 7720),\n ('more', 7521),\n ('when', 7456),\n ('some', 7441),\n ('if', 7285),\n ('just', 7152),\n ('can', 7001),\n ('story', 6780),\n ('time', 6515),\n ('my', 6488),\n ('great', 6419),\n ('well', 6405),\n ('up', 6321),\n ('which', 6267),\n ('their', 6107),\n ('see', 6026),\n ('also', 5550),\n ('we', 5531),\n ('really', 5476),\n ('would', 5400),\n ('will', 5218),\n ('me', 5167),\n ('had', 5148),\n ('only', 5137),\n ('him', 5018),\n ('even', 4964),\n ('most', 4864),\n ('other', 4858),\n ('were', 4782),\n ('first', 4755),\n ('than', 4736),\n ('much', 4685),\n ('its', 4622),\n ('no', 4574),\n ('into', 4544),\n ('people', 4479),\n ('best', 4319),\n ('love', 4301),\n ('get', 4272),\n ('how', 4213),\n ('life', 4199),\n ('been', 4189),\n ('because', 4079),\n ('way', 4036),\n ('do', 3941),\n ('made', 3823),\n ('films', 3813),\n ('them', 3805),\n ('after', 3800),\n ('many', 3766)]"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "[('', 561462),\n ('.', 167538),\n ('the', 163389),\n ('a', 79321),\n ('and', 74385),\n ('of', 69009),\n ('to', 68974),\n ('br', 52637),\n ('is', 50083),\n ('it', 48327),\n ('i', 46880),\n ('in', 43753),\n ('this', 40920),\n ('that', 37615),\n ('s', 31546),\n ('was', 26291),\n ('movie', 24965),\n ('for', 21927),\n ('but', 21781),\n ('with', 20878),\n ('as', 20625),\n ('t', 20361),\n ('film', 19218),\n ('you', 17549),\n ('on', 17192),\n ('not', 16354),\n ('have', 15144),\n ('are', 14623),\n ('be', 14541),\n ('he', 13856),\n ('one', 13134),\n ('they', 13011),\n ('at', 12279),\n ('his', 12147),\n ('all', 12036),\n ('so', 11463),\n ('like', 11238),\n ('there', 10775),\n ('just', 10619),\n ('by', 10549),\n ('or', 10272),\n ('an', 10266),\n ('who', 9969),\n ('from', 9731),\n ('if', 9518),\n ('about', 9061),\n ('out', 8979),\n ('what', 8422),\n ('some', 8306),\n ('no', 8143),\n ('her', 7947),\n ('even', 7687),\n ('can', 7653),\n ('has', 7604),\n ('good', 7423),\n ('bad', 7401),\n ('would', 7036),\n ('up', 6970),\n ('only', 6781),\n ('more', 6730),\n ('when', 6726),\n ('she', 6444),\n ('really', 6262),\n ('time', 6209),\n ('had', 6142),\n ('my', 6015),\n ('were', 6001),\n ('which', 5780),\n ('very', 5764),\n ('me', 5606),\n ('see', 5452),\n ('don', 5336),\n ('we', 5328),\n ('their', 5278),\n ('do', 5236),\n ('story', 5208),\n ('than', 5183),\n ('been', 5100),\n ('much', 5078),\n ('get', 5037),\n ('because', 4966),\n ('people', 4806),\n ('then', 4761),\n ('make', 4722),\n ('how', 4688),\n ('could', 4686),\n ('any', 4658),\n ('into', 4567),\n ('made', 4541),\n ('first', 4306),\n ('other', 4305),\n ('well', 4254),\n ('too', 4174),\n ('them', 4165),\n ('plot', 4154),\n ('movies', 4080),\n ('acting', 4056),\n ('will', 3993),\n ('way', 3989),\n ('most', 3919)]"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(positive_words_cnt.most_common(100))\n",
    "display(negative_words_cnt.most_common(100))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "- we have now the count of each word in both positive and negative, but we want the words that appear in positive only and doesn't appear in negative and vice versa\n",
    "- away for this to calculate the ration of the word log(positive_cnt/negative_cnt) if positive~negative then it's value will be ~0 if it appears in positive it will be large number greater than 0 and for negative it will be large negative number"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "words_pos_neg_ratio = Counter()\n",
    "for word, cnt in all_words_cnt.items():\n",
    "    if cnt < 100:  #doesn't appear much\n",
    "        continue\n",
    "    words_pos_neg_ratio[word] = np.log((positive_words_cnt[word] + 1) / (negative_words_cnt[word] + 1))\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "common words\n",
      "pos and negative ratio for 'the' 0.05902846378582202\n",
      "pos and negative ratio for 'is' 0.13365617519106257\n",
      "\n",
      "Postive words\n",
      "pos and negative ratio for 'wonderful' 1.56527119000835\n",
      "pos and negative ratio for 'wonderful' 1.3929263134236418\n",
      "\n",
      "negative words\n",
      "pos and negative ratio for 'bad' -1.3556946609378695\n",
      "pos and negative ratio for 'bad' -1.7327361287785938\n"
     ]
    }
   ],
   "source": [
    "print(\"common words\")\n",
    "print(f\"pos and negative ratio for 'the' {words_pos_neg_ratio['the']}\")\n",
    "print(f\"pos and negative ratio for 'is' {words_pos_neg_ratio['is']}\")\n",
    "print(\"\\nPostive words\")\n",
    "\n",
    "print(f\"pos and negative ratio for 'wonderful' {words_pos_neg_ratio['wonderful']}\")\n",
    "print(f\"pos and negative ratio for 'wonderful' {words_pos_neg_ratio['amazing']}\")\n",
    "\n",
    "print(\"\\nnegative words\")\n",
    "print(f\"pos and negative ratio for 'bad' {words_pos_neg_ratio['bad']}\")\n",
    "print(f\"pos and negative ratio for 'bad' {words_pos_neg_ratio['worse']}\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "common words ~ 0\n",
    "positive words >1\n",
    "negative words <-1\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positive most common\n"
     ]
    },
    {
     "data": {
      "text/plain": "[('edie', 4.700480365792417),\n ('paulie', 4.085976312551584),\n ('felix', 3.1612467120315646),\n ('polanski', 2.833213344056216),\n ('matthau', 2.8134107167600364),\n ('victoria', 2.6855773452501515),\n ('mildred', 2.6119063405493077),\n ('gandhi', 2.5477075510270306),\n ('flawless', 2.4595888418037104),\n ('superbly', 2.268683541318364),\n ('perfection', 2.1671471220989416),\n ('astaire', 2.1484344131667874),\n ('captures', 2.0438143640366846),\n ('voight', 2.0402208285265546),\n ('wonderfully', 2.02537432040956),\n ('powell', 1.9836504770381602),\n ('brosnan', 1.9636097261547143),\n ('lily', 1.9289605907415401),\n ('bakshi', 1.911718784307034),\n ('lincoln', 1.9079309009900969),\n ('refreshing', 1.8607523407150064),\n ('lemmon', 1.8562979903656263),\n ('breathtaking', 1.8549383708495866),\n ('bourne', 1.8538912503350613),\n ('flynn', 1.807507826196194),\n ('delightful', 1.8044984950054848),\n ('andrews', 1.7841548698428356),\n ('homer', 1.7805861686299298),\n ('soccer', 1.7692866133759964),\n ('beautifully', 1.76537271405486),\n ('lumet', 1.7578579175523736),\n ('elvira', 1.7473077066572211),\n ('underrated', 1.7247487589450947),\n ('gripping', 1.7243181884325225),\n ('superb', 1.71090737259896),\n ('delight', 1.6789639750827108),\n ('sadness', 1.6739764335716716),\n ('welles', 1.6724127115954888),\n ('sinatra', 1.6438393391514328),\n ('touching', 1.6399534563600509),\n ('timeless', 1.6389967146756448),\n ('macy', 1.6326947745983675),\n ('unforgettable', 1.6259672143853108),\n ('favorites', 1.6222586008631619),\n ('hartley', 1.6211339521972916),\n ('extraordinary', 1.6163107917218624),\n ('sullivan', 1.6156684621847364),\n ('stewart', 1.6145530131008707),\n ('brilliantly', 1.5998684614179497),\n ('friendship', 1.5720115069149834),\n ('palma', 1.5664205273504097),\n ('wonderful', 1.56527119000835),\n ('magnificent', 1.5512559570513647),\n ('finest', 1.550597412411167),\n ('ritter', 1.5493339883643948),\n ('jackie', 1.5491181222005777),\n ('tremendous', 1.5279448781829175),\n ('freedom', 1.5141277326297755),\n ('fantastic', 1.5063736090366242),\n ('terrific', 1.505482878385009),\n ('sidney', 1.5007047122976347),\n ('pleasantly', 1.4992347723004862),\n ('mann', 1.4992347723004862),\n ('noir', 1.4968362355197145),\n ('outstanding', 1.493925025312256),\n ('nancy', 1.4934389985712184),\n ('marie', 1.4873904779912595),\n ('marvelous', 1.4816045409242156),\n ('ruth', 1.4696759700589417),\n ('excellent', 1.4653478511838256),\n ('stanwyck', 1.4488147181012245),\n ('widmark', 1.442989704796436),\n ('splendid', 1.4370666864933137),\n ('chan', 1.4291143583028187),\n ('exceptional', 1.4284947156102672),\n ('tender', 1.423108334242607),\n ('gentle', 1.418382675671391),\n ('poignant', 1.410199881973445),\n ('gem', 1.3966571481554373),\n ('captivating', 1.3966571481554373),\n ('fisher', 1.3958638121360414),\n ('davies', 1.3958638121360414),\n ('chilling', 1.3936204012119635),\n ('amazing', 1.3929263134236418),\n ('darker', 1.3758230612525952),\n ('april', 1.3621968095408301),\n ('blake', 1.3531421538029902),\n ('kelly', 1.3490940774338),\n ('overlooked', 1.3388921222253067),\n ('ralph', 1.3366974199805186),\n ('bette', 1.3237740041385566),\n ('hoffman', 1.3217558399823195),\n ('cole', 1.3217558399823195),\n ('shines', 1.3133875903118029),\n ('powerful', 1.3020133612531182),\n ('notch', 1.3013949173334043),\n ('winters', 1.295322582914164),\n ('pitt', 1.292768303109067),\n ('remarkable', 1.2924756059022358),\n ('vivid', 1.2878542883066382)]"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"Positive most common\")\n",
    "display(words_pos_neg_ratio.most_common(100))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "words like magnificent and amazing, wonderful, appear from top 100"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Negative most common\n"
     ]
    },
    {
     "data": {
      "text/plain": "[('boll', -4.276666119016055),\n ('uwe', -3.9318256327243257),\n ('seagal', -3.4210000089583352),\n ('unwatchable', -3.0349529867072724),\n ('stinker', -2.9856819377004897),\n ('mst', -2.8449093838194073),\n ('incoherent', -2.803360380906535),\n ('unfunny', -2.635081181235619),\n ('waste', -2.6093342281630525),\n ('blah', -2.501435951739211),\n ('pointless', -2.430613567421338),\n ('horrid', -2.379546134130174),\n ('atrocious', -2.36528368720961),\n ('redeeming', -2.3331477434042123),\n ('worst', -2.283027494964281),\n ('prom', -2.2655438213136967),\n ('drivel', -2.26002547857525),\n ('lousy', -2.2587824703356527),\n ('laughable', -2.2396712675834767),\n ('awful', -2.22125951150762),\n ('poorly', -2.206570439754457),\n ('remotely', -2.145931282948669),\n ('wasting', -2.1400661634962708),\n ('existent', -2.02537432040956),\n ('lame', -1.970717622759581),\n ('sucks', -1.9580806846755685),\n ('insult', -1.94060509682562),\n ('boredom', -1.9379419794061366),\n ('miserably', -1.927891643552635),\n ('uninspired', -1.9187591599893623),\n ('uninteresting', -1.9066894359020319),\n ('horrible', -1.9038282036209997),\n ('pathetic', -1.8841327893590702),\n ('godzilla', -1.8827312474337816),\n ('unconvincing', -1.8748743759385615),\n ('amateurish', -1.8744511850731684),\n ('appalling', -1.8672670217362002),\n ('gadget', -1.8666607774011728),\n ('idiotic', -1.8484548129046003),\n ('unintentional', -1.845826690498331),\n ('stupidity', -1.836211231798889),\n ('wasted', -1.82537608002704),\n ('crap', -1.821206670554356),\n ('cardboard', -1.791759469228055),\n ('tedious', -1.7707060600302227),\n ('insulting', -1.7650912221458936),\n ('dreadful', -1.7443572303334711),\n ('badly', -1.743474228068593),\n ('worse', -1.7327361287785938),\n ('terrible', -1.7250680947293828),\n ('suck', -1.7147984280919266),\n ('dire', -1.709521370991083),\n ('mess', -1.6801454845983868),\n ('embarrassing', -1.6739764335716716),\n ('garbage', -1.6731199024700552),\n ('stupid', -1.6516020715417972),\n ('pile', -1.6384254493073527),\n ('vampires', -1.5950491749820006),\n ('ashamed', -1.5869650565820417),\n ('dull', -1.5775048653310912),\n ('worthless', -1.5723966407537513),\n ('avoid', -1.568304924286708),\n ('wooden', -1.552685095841651),\n ('inept', -1.5496194172231903),\n ('forgettable', -1.5248805244060373),\n ('crappy', -1.5141277326297755),\n ('ridiculous', -1.502813972729502),\n ('bat', -1.5004867286455454),\n ('fulci', -1.498772344546581),\n ('excuse', -1.4968362355197145),\n ('whatsoever', -1.494616588272045),\n ('rubbish', -1.48870936654796),\n ('boring', -1.4879804697340995),\n ('unbelievably', -1.4733057381095205),\n ('junk', -1.466337068793427),\n ('turkey', -1.4593194961347804),\n ('shark', -1.449095262358921),\n ('flop', -1.4441139320087168),\n ('topless', -1.4350845252893227),\n ('useless', -1.4350845252893227),\n ('ripped', -1.4315509527080115),\n ('ridiculously', -1.4307461236907244),\n ('embarrassed', -1.4246132254220272),\n ('seed', -1.423108334242607),\n ('costs', -1.418382675671391),\n ('dumb', -1.415281897993143),\n ('bother', -1.4116121691041805),\n ('rambo', -1.3971052772241062),\n ('horrendous', -1.3955110162248145),\n ('horribly', -1.3920914788042167),\n ('plastic', -1.3862943611198906),\n ('hideous', -1.3862943611198906),\n ('fest', -1.3773256911371303),\n ('disjointed', -1.374318170073175),\n ('ludicrous', -1.3723081191451507),\n ('bland', -1.3682758556172123),\n ('annoying', -1.3614791920001665),\n ('unintentionally', -1.3591433720539396),\n ('mildly', -1.3581234841531944),\n ('obnoxious', -1.3564413979702095)]"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"Negative most common\")\n",
    "display(list(reversed(words_pos_neg_ratio.most_common()))[0:100])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max value:4.700480365792417\n",
      "min value:-4.276666119016055\n",
      "Words count:74074\n"
     ]
    }
   ],
   "source": [
    "print(f\"max value:{helper.max_counter_value(words_pos_neg_ratio)}\")\n",
    "print(f\"min value:{helper.min_counter_value(words_pos_neg_ratio)}\")\n",
    "print(f\"Words count:{len(all_words_cnt)}\")\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Creating the Model\n",
    "- ***Network structure***\n",
    "![network](notebook-images/network.png)\n",
    "- ---\n",
    "- after viewing the data existence of some words in a review could classify it to positive or negative so our network will be as follows\n",
    "- we need to represent the data input as vector of zeros and ones when the word exists in a review it will have value one in the index of the word in the vector\n",
    "- we need to ignore irrelevant words that appear in both positive and negative via threshold in pos_negative_ratio"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "outputs": [],
   "source": [
    "class Model:\n",
    "    def __init__(self, reviews, labels, hidden_nodes=128, learning_rate=0.1, threshold=0.8):\n",
    "        np.random.seed(69)\n",
    "        self.hidden_nodes = hidden_nodes\n",
    "        self.learning_rate = learning_rate\n",
    "        self.threshold = threshold\n",
    "\n",
    "        filtered_vocab = self.data_preprocessing(reviews, labels)\n",
    "        self.word2idx = {}\n",
    "        for idx, word in enumerate(filtered_vocab):\n",
    "            self.word2idx[word] = idx\n",
    "        self.input_size = len(filtered_vocab)\n",
    "        self.fc1 = helper.NNLiner(self.input_size, self.hidden_nodes)\n",
    "        # self.fc1.weights=np.zeros((self.input_size,self.hidden_nodes))\n",
    "        self.fc2 = helper.NNLiner(self.hidden_nodes, 1)\n",
    "\n",
    "        self.sigmoid = lambda x: 1.0 / (1.0 + (np.exp(-x)))\n",
    "\n",
    "        self.crossEntropy = lambda y, p: -(y * np.log(p) + (1 - y) * np.log(1 - p))\n",
    "        # self.squared_error=lambda y,p: -(y*np.log(p)+(1-y)*np.log(1-p))\n",
    "\n",
    "    def review_encode(self, review):\n",
    "        a0 = np.zeros(self.input_size, float)\n",
    "        for word in review.split(' '):\n",
    "            if word in self.word2idx:\n",
    "                a0[self.word2idx[word]] = 1.0\n",
    "        return np.expand_dims(a0, axis=0)\n",
    "\n",
    "    def train(self, epochs, train_reviews, train_labels,test_reviews,test_lables):\n",
    "\n",
    "        assert (len(train_reviews) == len(train_labels))\n",
    "        correct_cnt = 0\n",
    "        forward_cnt = 0\n",
    "        print_every=int(len(train_labels)*0.1) #print log every 10 percent\n",
    "        start_time = time.time()\n",
    "        train_accuracy=0\n",
    "        data_size=len(train_labels)\n",
    "\n",
    "        for _ in range(epochs):\n",
    "            for review, label in zip(train_reviews, train_labels):\n",
    "                forward_cnt += 1\n",
    "                if label == 'positive':\n",
    "                    actual_output = 1\n",
    "                else:\n",
    "                    actual_output = 0\n",
    "\n",
    "                #------------------------------Feed-Forward----------------------------------------#\n",
    "                a0 = self.review_encode(review)\n",
    "                z1 = self.fc1(a0)\n",
    "                # a1=self.sigmoid(z1)\n",
    "                a1 = z1\n",
    "\n",
    "                z2 = self.fc2(a1)\n",
    "                a2 = self.sigmoid(z2)\n",
    "\n",
    "                predicted = 1\n",
    "                if a2 < 0.5:\n",
    "                    predicted = 0\n",
    "                if predicted == actual_output:\n",
    "                    correct_cnt += 1\n",
    "                #d --> derivative\n",
    "\n",
    "                error = (1 / 2) * (actual_output - a2) ** 2 # 2d matrix [[error]]\n",
    "                error=error[0][0]\n",
    "                #______________________________Backpropagation_______________________________________#\n",
    "                #Ti --> error term of layer i\n",
    "                #Ti+1=Ti @\n",
    "\n",
    "                # _______________delta_w2____________________\n",
    "                #         |  error term 2  |\n",
    "                # dE/dW2 = dE/a2 * da2/dz2 * dz2/dw2 = T2 * da2/dz2\n",
    "                de_da2 = -(actual_output - a2)\n",
    "                da2_dz2 = a2 * (1 - a2)  # derivative of sigmoid is sigmoid*(1-sigmoid)\n",
    "\n",
    "                t2 = de_da2 * da2_dz2  # --> dimensions (1x2) = dimensions of a2\n",
    "\n",
    "                de_dw2 = (a1.transpose()) @ t2  #--> (3*1)@ (1*2) = (3x2) same like w2 size\n",
    "                #-----------------------------------------------\n",
    "\n",
    "                # _______________delta_w2____________________\n",
    "                #dE/dW1=dE/a2 * da2/dz2 * dz2/da1 * da1/dz1 * dz1/dw1 = t2 * dz2/da1 * da1/dz1 * dz1/dw1\n",
    "\n",
    "                #dz2/da1= d/da1 (a1@W2 + b2) = W2\n",
    "                # da1_z1=a1*(1-a1)\n",
    "                da1_z1 = 1\n",
    "                #dz1/dw1=a0\n",
    "\n",
    "                t1 = t2 @ self.fc2.weights.T * da1_z1\n",
    "                de_dw1 = a0.transpose() @ t1\n",
    "\n",
    "                #------------ update weights ------------------#\n",
    "                self.fc1.weights -= (de_dw1 * self.learning_rate)\n",
    "                self.fc2.weights -= (de_dw2 * self.learning_rate)\n",
    "                #-----------------------------------------------\n",
    "\n",
    "                elapsed_time = float(time.time() - start_time)\n",
    "                reviews_per_second = forward_cnt / elapsed_time if elapsed_time > 0 else 0\n",
    "\n",
    "                train_accuracy=(correct_cnt * 100) / float(forward_cnt + 1)\n",
    "\n",
    "                sys.stdout.write(\"\\rProgress:\" + str(100 * forward_cnt / float(data_size))[:4]\n",
    "                                 + \"% Speed(reviews/sec):\" + str(reviews_per_second)[0:5]\n",
    "                                 + \" #Correct:\" + str(correct_cnt) + \" #Trained:\" + str(forward_cnt + 1)\n",
    "                                 + \" Training Accuracy:\" + str(train_accuracy)[:4] + \"%\"+\n",
    "                                 \" Train loss: \"+str(error)[:8])\n",
    "                if forward_cnt % print_every == 0:\n",
    "                    print(\"\")\n",
    "            print(\"\\nTesting\\n\")\n",
    "            #------------------------------------------------------Testing----------------------------------------------------------#\n",
    "            correct_cnt = 0\n",
    "            forward_cnt = 0\n",
    "            start_time = time.time()\n",
    "            data_size=len(test_labels)\n",
    "            print_every=int(len(test_lables)*0.1) #print log every 10 percent\n",
    "            test_accuracy=0\n",
    "\n",
    "            for review, label in zip(test_reviews, test_lables):\n",
    "                forward_cnt += 1\n",
    "                if label == 'positive':\n",
    "                    actual_output = 1\n",
    "                else:\n",
    "                    actual_output = 0\n",
    "                #------------------------------Feed-Forward----------------------------------------#\n",
    "                a2=self(review)\n",
    "                error = (1 / 2) * (actual_output - a2) ** 2 # 2d matrix [[error]]\n",
    "                error=error[0][0]\n",
    "\n",
    "                predicted = 1\n",
    "                if a2 < 0.5:\n",
    "                    predicted = 0\n",
    "                if predicted == actual_output:\n",
    "                    correct_cnt += 1\n",
    "                elapsed_time = float(time.time() - start_time)\n",
    "                reviews_per_second = forward_cnt / elapsed_time if elapsed_time > 0 else 0\n",
    "                test_accuracy=(correct_cnt * 100) / float(forward_cnt + 1)\n",
    "\n",
    "\n",
    "                sys.stdout.write(\"\\rProgress:\" + str(100 * forward_cnt / float(data_size))[:4]\n",
    "                                 + \"% Speed(reviews/sec):\" + str(reviews_per_second)[0:5]\n",
    "                                 + \" #Correct:\" + str(correct_cnt) + \" #Tested:\" + str(forward_cnt + 1)\n",
    "                                 + \" Testing Accuracy:\" + str(test_accuracy)[:4] + \"%\"+\n",
    "                                 \" Test loss: \"+str(error)[:8])\n",
    "                if forward_cnt % print_every == 0:\n",
    "                    print(\"\")\n",
    "            if train_accuracy>test_accuracy:\n",
    "                print(\"\\nWarning overfitting training loop will break\")\n",
    "                break\n",
    "\n",
    "\n",
    "    def data_preprocessing(self, reviews, labels):\n",
    "        # calculate the importance of each word using pos\n",
    "        filtered_reviews_vocab = set()\n",
    "        positive_words_cnt = Counter()\n",
    "        negative_words_cnt = Counter()\n",
    "        reviews_vocab = set()\n",
    "\n",
    "        for i in range(len(reviews)):\n",
    "            words = reviews[i].split(' ')\n",
    "            for word in words:\n",
    "                if labels[i] == 'positive':\n",
    "                    positive_words_cnt[word] += 1\n",
    "                else:\n",
    "                    negative_words_cnt[word] += 1\n",
    "                reviews_vocab.add(word)\n",
    "\n",
    "        for word in reviews_vocab:\n",
    "            word_ratio = np.log((positive_words_cnt[word] + 1) / (negative_words_cnt[word] + 1))\n",
    "            if abs(word_ratio) >= self.threshold:\n",
    "                filtered_reviews_vocab.add(word)\n",
    "        return filtered_reviews_vocab\n",
    "\n",
    "    def __call__(self, review):\n",
    "        a0 = self.review_encode(review)\n",
    "\n",
    "        z1 = self.fc1(a0)\n",
    "        # a1 = self.sigmoid(z1)\n",
    "        a1=z1\n",
    "        z2 = self.fc2(a1)\n",
    "        a2 = self.sigmoid(z2)\n",
    "        return a2"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "outputs": [],
   "source": [
    "#test\n",
    "model = Model(reviews, labels, learning_rate=0.001, hidden_nodes=25, threshold=0.15)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "x = 0\n",
    "print(model.__call__(reviews[x]))\n",
    "print(labels[x])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "**data is evenly distributed data even indices for positive and  odd for negative**"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# since data is evently distributed we could split by index\n",
    "\n",
    "test_percentage = 0.25\n",
    "\n",
    "train_reviews,test_reviews,train_labels,test_labels=train_test_split(reviews,labels,test_size=test_percentage,random_state=69)\n",
    "data_size = int(len(reviews)*0.10)\n",
    "\n",
    "print(f\"total records={data_size}\\ntrain_size={len(train_reviews)}\\ntest_size={len(test_reviews)}\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model.train(1, train_reviews, train_labels,test_reviews,test_labels)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alain chabat claims this movie as his original idea but the theme of reluctant lovers who finally get it together is as old  if not older  than shakespeare .  br    br   chabat is a  vieux garcon   happily single and not wanting any member of the opposite sex to disturb his life . he has a problem   sisters and a matriarchal mum  the g   who decide he should be married . enter the delightful  charming charlotte gainsbourg and what should be a simple plan . charlotte has to pose as chabat  s girl\n",
      "model output [[0.49804155]]\n",
      "predicted negative\n",
      "actual positive\n"
     ]
    }
   ],
   "source": [
    "idx=np.random.randint(0,len(reviews))\n",
    "print(reviews[idx][:500])\n",
    "output=model(reviews[idx])\n",
    "predicted=\"positive\"\n",
    "if output<0.5:\n",
    "    predicted=\"negative\"\n",
    "print(f\"model output {output}\")\n",
    "print(f\"predicted {predicted}\")\n",
    "print(f\"actual {labels[idx]}\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}